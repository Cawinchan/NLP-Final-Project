{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import conlleval\n",
    "\n",
    "train_dir = \"dataset/train\"\n",
    "test_dir = \"dataset/dev.in\"\n",
    "\n",
    "START_STATE_KEY = \"START\"\n",
    "STOP_STATE_KEY = \"STOP\"\n",
    "\n",
    "LARGE_NEG = -2**52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_path):  \n",
    "    data, lst = [], []\n",
    "    with open(file_path, 'r') as f:  \n",
    "        for line in f:\n",
    "            if line== '\\n':\n",
    "                data.append(lst)\n",
    "                lst = []    \n",
    "            else:\n",
    "                lines = line.replace(\"\\n\",'').split(\" \")\n",
    "                lst.append(tuple(lines))\n",
    "    return data\n",
    "\n",
    "train_sentences = tokenize(train_dir)\n",
    "print(train_sentences[:5])      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "## 1i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MLE_emission_parameters(train_sentences):\n",
    "    ''' Calculates the emission parameters by count(y->x)/count(y)\n",
    "    \n",
    "    :param train_sentences: our train file tokenised sentences\n",
    "    :type train_sentences: list(tuple())\n",
    "\n",
    "    :return count_y_dict: Count of labels \n",
    "    :rtype: dict()\n",
    "\n",
    "    :return count_y_to_x_dict: Count of words and labels\n",
    "    :rtype: dict()\n",
    "\n",
    "    :param emission_dict: value of Count(labels->words)/Count(labels), keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict\n",
    "\n",
    "    '''\n",
    "\n",
    "    count_y_dict = {}\n",
    "    count_y_to_x_dict = {}\n",
    "    emission_dict = {}\n",
    "    word_bank = []\n",
    "\n",
    "    for sentence in train_sentences:\n",
    "        for x_y_pair in sentence:\n",
    "            word, label = x_y_pair\n",
    "            word_bank.append(word)\n",
    "            count_y_dict[label] = count_y_dict.get(label,0) + 1\n",
    "            count_y_to_x_dict[(label,word)] = count_y_to_x_dict.get((label,word),0) + 1\n",
    "\n",
    "    # Calculate our emission\n",
    "    for key, value in count_y_to_x_dict.items(): \n",
    "        label = key[0]\n",
    "        word = key[1]\n",
    "        string = f\"emission: {label}+{word}\" \n",
    "        prob =  value / count_y_dict.get(label)\n",
    "        emission_dict[string] = float(np.where(prob != 0, np.log(prob), 0))\n",
    "\n",
    "    # handle missing possible words and labels\n",
    "    unique_word_bank, labels = set(word_bank), count_y_dict.keys()\n",
    "    for label in labels:       \n",
    "        for word in unique_word_bank:\n",
    "            string = f\"emission: {label}+{word}\"\n",
    "            if string not in emission_dict:\n",
    "                emission_dict[string] = LARGE_NEG\n",
    "\n",
    "    return count_y_dict, count_y_to_x_dict, emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_y_dict, count_y_to_x_dict, emission_dict = MLE_emission_parameters(train_sentences)\n",
    "print(list(emission_dict.items())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  MLE_transition_parameters(train_dir, emission_dict):\n",
    "    ''' Calculates the transition parameters by count(y->y-1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: Count(y->x)/Count(y), keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict()\n",
    "    '''\n",
    "    count_y_dict = {}\n",
    "    count_y_to_y_dict = {}\n",
    "    prev_label = \"\"\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                _, label = line.replace(\"\\n\",\"\").split(\" \")\n",
    "            else:\n",
    "                label = ''\n",
    "            if label == '' and prev_label != '':\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY) + 1 if count_y_dict.get(STOP_STATE_KEY) else 1\n",
    "            elif label !='':\n",
    "                if prev_label == '':\n",
    "                    count_y_dict[START_STATE_KEY] = count_y_dict.get(START_STATE_KEY) + 1 if count_y_dict.get(START_STATE_KEY) else 1\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label)+1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "            if prev_label == '' and label != '':\n",
    "                if (START_STATE_KEY, label) in count_y_to_y_dict:\n",
    "                    count_y_to_y_dict[(START_STATE_KEY, label)] = count_y_to_y_dict.get((START_STATE_KEY, label)) + 1\n",
    "                else:\n",
    "                    count_y_to_y_dict[(START_STATE_KEY, label)] = 1\n",
    "            elif label == '' and prev_label != '':\n",
    "                if (prev_label, STOP_STATE_KEY) in count_y_to_y_dict:\n",
    "                    count_y_to_y_dict[(prev_label, STOP_STATE_KEY)] = count_y_to_y_dict.get((prev_label, STOP_STATE_KEY)) + 1\n",
    "                else:\n",
    "                    count_y_to_y_dict[(prev_label, STOP_STATE_KEY)] = 1\n",
    "            elif label != '' and prev_label != '':\n",
    "                if (prev_label, label) in count_y_to_y_dict:\n",
    "                    count_y_to_y_dict[(prev_label, label)] = count_y_to_y_dict.get((prev_label, label)) + 1\n",
    "                else:\n",
    "                    count_y_to_y_dict[(prev_label, label)] = 1\n",
    "            prev_label = label\n",
    "\n",
    "    # Calculate our transition\n",
    "    for key, value in count_y_to_y_dict.items(): # Default is iterate keys()\n",
    "        prev_label = key[0]\n",
    "        label = key[1]\n",
    "        string = f\"transition: {prev_label}+{label}\" \n",
    "        prob =  value / count_y_dict.get(prev_label)\n",
    "        emission_transition_dict[string] = float(np.where(prob != 0, np.log(prob), 0))\n",
    "    # print(\"MLE: \\n\",list(emission_transition_dict.items()), len(emission_transition_dict) ,\"\\n\")\n",
    "\n",
    "    labels = count_y_dict.keys()\n",
    "    for label in labels:\n",
    "        for prev_label in labels:\n",
    "            string = f\"transition: {prev_label}+{label}\" \n",
    "            if string not in emission_transition_dict:\n",
    "                emission_transition_dict[string] = LARGE_NEG\n",
    "\n",
    "    return count_y_to_y_dict, emission_transition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_y_to_y_dict, emission_transition_dict = MLE_transition_parameters(train_dir, emission_dict)\n",
    "print(list(emission_transition_dict.items())[:5])\n",
    "print(list(emission_transition_dict.items())[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sentence, emission_transition_dict):\n",
    "    ''' Calculates the score with of a given pair based on emission and transmission features\n",
    "    \n",
    "    :param sentences: our  file tokenised sentences\n",
    "    :type sentences: list(tuple())\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :type emission_transition_dict: dict()\n",
    "\n",
    "    :param score: our emission score + transition score for sentence\n",
    "    :type sentences: float\n",
    "    '''\n",
    "    score = 0\n",
    "    emission_score = 0 \n",
    "    transition_score = 0\n",
    "    x_seq = [x[0] for x in sentence]\n",
    "    y_seq = [START_STATE_KEY]+[y[1] for y in sentence]+[STOP_STATE_KEY]\n",
    "    \n",
    "    for i in range(len(x_seq)):\n",
    "        label = y_seq[i+1]\n",
    "        word = x_seq[i]\n",
    "        key = f\"emission: {label}+{word}\" \n",
    "        emission_score += emission_transition_dict[key]\n",
    "    for j in range(1, len(y_seq)):\n",
    "        prev_label = y_seq[j-1]\n",
    "        label = y_seq[j]\n",
    "        key = f\"transition: {prev_label}+{label}\" \n",
    "        transition_score += emission_transition_dict[key]\n",
    "    score = emission_score + transition_score\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(train_sentences[0],emission_transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = tokenize(test_dir)\n",
    "print(test_sentences[:5])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(test_sentences, count_y_dict, emission_transition_dict, output_name):\n",
    "    ''' Decoding process that finds globally finds the best possible labels from past MLE scores, saves file to output folder\n",
    "    \n",
    "    :param test_sentences: our file tokenised sentences\n",
    "    :type test_sentences: list(tuple())\n",
    "\n",
    "    :param count_y_dict: Count of labels \n",
    "    :param count_y_dict: dict()\n",
    "\n",
    "    :param emission_transition_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :param emission_transition_dict: dict()\n",
    "    '''\n",
    "    \n",
    "    pi = [{}]\n",
    "    path = {}\n",
    "    labels = count_y_dict.keys()\n",
    "    os.makedirs('output',exist_ok=True)\n",
    "\n",
    "    with open(f'output/{output_name}', \"w\") as outfile:\n",
    "        for sentence in test_sentences:\n",
    "            # j = 0 (START)\n",
    "            for label in labels:\n",
    "                pi[0][label] = emission_transition_dict.get(f\"transition: {START_STATE_KEY}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[0][0]}\",LARGE_NEG)\n",
    "                path[label] = [label]\n",
    "            # j = 1 to N-1\n",
    "            for idx in range(1,len(sentence)):\n",
    "                pi.append({})\n",
    "                newpath = {}\n",
    "                for label_y in labels:\n",
    "                    (prob, label) = max([(pi[idx-1][prev_label] + emission_transition_dict.get(f\"transition: {prev_label}+{label_y}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label_y}+{sentence[idx][0]}\",LARGE_NEG), prev_label) \n",
    "                                    for prev_label in labels])\n",
    "                    pi[idx][label_y] = prob\n",
    "                    newpath[label_y] = path[label] + [label_y]\n",
    "                path = newpath\n",
    "            # j = N (STOP)\n",
    "            idx = len(sentence)\n",
    "            (prob, label) = max([(pi[idx-1][label_y] + emission_transition_dict.get(f\"transition: {label_y}+{STOP_STATE_KEY}\", LARGE_NEG), label_y) for label_y in labels])\n",
    "            \n",
    "            # handle inconsistent length\n",
    "            if len(sentence) != len(path[label]):\n",
    "                print(len(sentence),len(path[label]))\n",
    "                raise Exception(\"{} has a different lenght with {}\".format(sentence, path[label]))\n",
    "            \n",
    "            # write to file\n",
    "            for i in range(len(sentence)):\n",
    "                line = \"{} {}\\n\".format(sentence[i][0], path[label][i])\n",
    "                outfile.write(line)\n",
    "                \n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_algo(test_sentences, count_y_dict, emission_transition_dict, output_name='dev.p2.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of dev.p2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dir = 'output/dev.p2.out'\n",
    "truth_dir = 'dataset/dev.out'\n",
    "\n",
    "def evaluate_results(truth_dir,prediction_dir):\n",
    "    predictions = []\n",
    "    prediction_sentences = tokenize(prediction_dir)\n",
    "    for sentence in prediction_sentences:\n",
    "        for word_pair in sentence:\n",
    "            predictions.append(word_pair[1])     \n",
    "    lines = \"\"\"\"\"\"\n",
    "    idx = 0\n",
    "    with open(truth_dir, \"r\", encoding=\"utf8\") as tstr:\n",
    "        for line in tstr:\n",
    "            if len(line) > 1:\n",
    "                newline = line.replace(\"\\n\",f\" {predictions[idx]}\\n\")\n",
    "                lines += newline\n",
    "                idx += 1\n",
    "            else:\n",
    "                lines += \"\\n\"\n",
    "    return lines.splitlines()\n",
    "\n",
    "lines = evaluate_results(truth_dir,prediction_dir)\n",
    "res = conlleval.evaluate(lines)\n",
    "print(conlleval.report(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "## 3i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logSumExp(a):\n",
    "    max = np.max(a)\n",
    "    sumOfExp = np.exp(a - max).sum()\n",
    "    return max + np.log(sumOfExp)\n",
    "\n",
    "def forward_algorithm(sentence, count_y_dict, emission_transition_dict):    \n",
    "    pi = [{}]\n",
    "    labels = count_y_dict.keys()\n",
    "    # j = 0 (START)\n",
    "    for label in labels:\n",
    "        pi[0][label] = emission_transition_dict.get(f\"transition: {START_STATE_KEY}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[0][0]}\",LARGE_NEG)\n",
    "\n",
    "    # j = 1 to N-1\n",
    "    for idx in range(1,len(sentence)):\n",
    "        pi.append({})\n",
    "\n",
    "        for label in labels:\n",
    "            log_a = []\n",
    "            for prev_label in labels:\n",
    "                log_a.append(pi[idx-1][prev_label] + emission_transition_dict.get(f\"transition: {prev_label}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[idx][0]}\",LARGE_NEG))\n",
    "            pi[idx][label] = logSumExp(log_a)\n",
    "            \n",
    "    # j = N (STOP)\n",
    "    idx = len(sentence)\n",
    "    log_a = []\n",
    "    for label in labels:\n",
    "        log_a.append(pi[idx-1][label] + emission_transition_dict.get(f\"transition: {label}+{STOP_STATE_KEY}\", LARGE_NEG))\n",
    "    return pi, logSumExp(log_a)\n",
    "\n",
    "forward_algorithm(train_sentences[0], count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(sentences, count_y_dict, emission_transition_dict):\n",
    "    loss = 0\n",
    "    for sent in sentences:\n",
    "        loss+= score(sent, emission_transition_dict)\n",
    "        _, update = forward_algorithm(sent, count_y_dict, emission_transition_dict)\n",
    "        loss-= update\n",
    "    return (-1)*loss\n",
    "loss_fn(train_sentences, count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_algorithm(sentence, count_y_dict, emission_transition_dict):    \n",
    "    pi = [{} for i in range(len(sentence))]\n",
    "    labels = count_y_dict.keys()\n",
    "\n",
    "    # j = N (STOP)\n",
    "    for label in labels:\n",
    "        pi[len(sentence)-1][label] = emission_transition_dict.get(f\"transition: {label}+{STOP_STATE_KEY}\", LARGE_NEG)\n",
    "\n",
    "    # j = N-1 to 1 \n",
    "    for idx in range(len(sentence)-1,0,-1):\n",
    "        for label in labels:\n",
    "            log_b = []\n",
    "            for next_label in labels:\n",
    "                log_b.append(pi[idx][next_label] + emission_transition_dict.get(f\"transition: {label}+{next_label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {next_label}+{sentence[idx][0]}\",LARGE_NEG))\n",
    "            pi[idx-1][label] = logSumExp(log_b)\n",
    "\n",
    "    # j = 0 (START)\n",
    "    log_b = []\n",
    "    for label in labels:\n",
    "        log_b.append(pi[0][label] + emission_transition_dict.get(f\"transition: {START_STATE_KEY}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[0][0]}\",LARGE_NEG))\n",
    "    \n",
    "    return pi, logSumExp(log_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_algorithm(train_sentences[0], count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_algorithm(sentence, count_y_dict, emission_transition_dict):\n",
    "    feature_expectation = {}\n",
    "    labels = count_y_dict.keys()\n",
    "    fwd_pi, fwd_score = forward_algorithm(sentence, count_y_dict, emission_transition_dict)\n",
    "    bkd_pi, bkd_score = backward_algorithm(sentence, count_y_dict, emission_transition_dict)\n",
    "        \n",
    "    # idx = 1\n",
    "    for label in labels:\n",
    "        string_transition = f\"transition: {START_STATE_KEY}+{label}\"\n",
    "        string_emission = f\"emission: {label}+{sentence[0][0]}\" \n",
    "\n",
    "        # update transition features\n",
    "        update = bkd_pi[0][label] \\\n",
    "                + emission_transition_dict[string_transition] \\\n",
    "                + emission_transition_dict[string_emission] \\\n",
    "                - fwd_score\n",
    "         \n",
    "        feature_expectation[string_transition] = feature_expectation.get(string_transition,0) + np.exp(update)\n",
    "        \n",
    "        # update emission features\n",
    "        update = fwd_pi[0][label] + bkd_pi[0][label] - fwd_score\n",
    "        feature_expectation[string_emission] = feature_expectation.get(string_emission,0) + np.exp(update)\n",
    "        \n",
    "    # idx = 2 to N-1\n",
    "    for idx in range(1,len(sentence)):\n",
    "        for label in labels:\n",
    "            string_emission = f\"emission: {label}+{sentence[idx][0]}\" \n",
    "\n",
    "            # update transition features\n",
    "            for prev_label in labels:\n",
    "                string_transition = f\"transition: {prev_label}+{label}\" \n",
    "                update = fwd_pi[idx-1][prev_label] \\\n",
    "                        + bkd_pi[idx][label] \\\n",
    "                        + emission_transition_dict[string_transition] \\\n",
    "                        + emission_transition_dict[string_emission] \\\n",
    "                        - fwd_score\n",
    "                feature_expectation[string_transition] = feature_expectation.get(string_transition,0) + np.exp(update)\n",
    "\n",
    "            # update emission features\n",
    "            update = fwd_pi[idx][label] + bkd_pi[idx][label] - fwd_score      \n",
    "            feature_expectation[string_emission] = feature_expectation.get(string_emission,0) + np.exp(update)\n",
    "                \n",
    "    # idx = N (STOP)\n",
    "    idx = len(sentence)\n",
    "    for label in labels:\n",
    "        # update transition features\n",
    "        string_transition = f\"transition: {label}+{STOP_STATE_KEY}\" \n",
    "        update = fwd_pi[idx-1][label] + emission_transition_dict[string_transition] - fwd_score\n",
    "        feature_expectation[string_transition] = feature_expectation.get(string_transition,0) + np.exp(update)\n",
    "        \n",
    "    return feature_expectation\n",
    "                \n",
    "forward_backward_algorithm(train_sentences[0], count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sentences, count_y_dict, emission_transition_dict):\n",
    "    features = {k:0 for k,v in emission_transition_dict.items()}\n",
    "    for sent in sentences:\n",
    "        expect = forward_backward_algorithm(sent, count_y_dict, emission_transition_dict)\n",
    "        for k,v in expect.items():\n",
    "            features[k] += v\n",
    "    return features\n",
    "\n",
    "features = get_features(train_sentences, count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fn(emission_transition_dict):\n",
    "    index_map = {}\n",
    "    for idx, value in enumerate(emission_transition_dict): \n",
    "        # print(value.split(\" \")[1].split(\"+\")[0], value.split(\" \")[1].split(\"+\")[1])\n",
    "        if len(value.split(\" \")[1].split(\"+\")) > 2:\n",
    "            first = str(value.split(\" \")[1].split(\"+\")[0])\n",
    "            second = '+'\n",
    "            print(idx)\n",
    "        else:\n",
    "            first = str(value.split(\" \")[1].split(\"+\")[0])\n",
    "            second = str(value.split(\" \")[1].split(\"+\")[1])\n",
    "        index_map[idx] = (first,second)\n",
    "    return index_map\n",
    "\n",
    "index_map = mapping_fn(emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(sentences, count_y_dict, features, index_map, emission_transition_dict):\n",
    "    labels = count_y_dict.keys()\n",
    "    counter = 0 \n",
    "    grad_lst = np.zeros(len(emission_transition_dict),)\n",
    "    for i in range(len(features)):\n",
    "        # print(index_map[i],count_y_to_x_dict.keys())\n",
    "        if index_map[i] in count_y_to_x_dict.keys():\n",
    "            string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            grad_lst[i] += (features[string] - count_y_to_x_dict[index_map[i]])\n",
    "        elif index_map[i] in count_y_to_y_dict.keys():\n",
    "            string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            grad_lst[i] += (features[string] - count_y_to_y_dict[index_map[i]])\n",
    "        else:\n",
    "            try:\n",
    "                string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "                grad_lst[i] += (features[string])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "                grad_lst[i] += (features[string])\n",
    "            except:\n",
    "                pass\n",
    "    return grad_lst\n",
    "\n",
    "compute_grad(train_sentences, count_y_dict, features, index_map, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4\n",
    "## 4i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_with_reg(w, sentences, count_y_dict, emission_transition_dict, n = 0.1):\n",
    "    loss = loss_fn(sentences, count_y_dict, emission_transition_dict)\n",
    "    # regularization\n",
    "    loss += n*sum(w1*w1 for w1 in w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_with_reg(w, sentences, count_y_dict, features, index_map, emission_transition_dict, n = 0.1):\n",
    "    grad_lst = compute_grad(sentences, count_y_dict, features, index_map, emission_transition_dict)\n",
    "    for i in range(len(w)):\n",
    "        grad_lst[i] += w[i]*2*n\n",
    "    return grad_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "total_start = time.time()\n",
    "\n",
    "def callbackF(w):\n",
    "    '''\n",
    "    This function will only be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    '''\n",
    "    loss = get_loss_grad(w)[0]\n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w):\n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    Returns:\n",
    "    loss: loss, float\n",
    "    grads: gradients, numpy array\n",
    "    '''\n",
    "    # to be completed by you\n",
    "    start = time.time()\n",
    "    new_emission_transition_dict = {}\n",
    "    for i in range(len(index_map.keys())):\n",
    "        if i < len(emission_dict):\n",
    "            string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            new_emission_transition_dict[string] = w[i]\n",
    "        else:\n",
    "            string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            new_emission_transition_dict[string] = w[i]\n",
    "    features = get_features(train_sentences, count_y_dict, new_emission_transition_dict)\n",
    "\n",
    "    loss = loss_with_reg(w, train_sentences, count_y_dict, new_emission_transition_dict, n = 0.1)\n",
    "    print('loss: '+ str(loss))\n",
    "    \n",
    "    grad_lst = grad_with_reg(w, train_sentences, count_y_dict, features, index_map, new_emission_transition_dict, n = 0.1)\n",
    "    #print('grad_lst: '+ str(grad_lst))\n",
    "    grads = np.asarray(list(grad_lst)) \n",
    "    \n",
    "    print('time taken: '+ str(time.time()-start) +' total time: '+ str(time.time()-total_start))\n",
    "    \n",
    "    return loss, grads\n",
    "  \n",
    "init_w = np.zeros(len(index_map),)\n",
    "results = fmin_l_bfgs_b(get_loss_grad, init_w, pgtol=0.01, maxiter=50, callback=callbackF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emission_transition_dict = {}\n",
    "for i in range(len(index_map.keys())): #27899\n",
    "    if i< len(emission_dict): #27818\n",
    "        string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "        new_emission_transition_dict[string] = results[0][i]\n",
    "    else:\n",
    "        string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "        new_emission_transition_dict[string] = results[0][i]\n",
    "\n",
    "viterbi_algo(test_sentences, count_y_dict, new_emission_transition_dict, 'dev.p4.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dir = 'output/dev.p4.out'\n",
    "truth_dir = 'dataset/dev.out'\n",
    "\n",
    "lines = evaluate_results(truth_dir,prediction_dir)\n",
    "res = conlleval.evaluate(lines)\n",
    "print(conlleval.report(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_1_parameters(train_dir, emission_dict):\n",
    "    \"\"\"Calculates the transition parameters by count(y->x_i-1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: count(y->x_i-1)/count(y), keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels->words_i-1)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict()\n",
    "    \"\"\"\n",
    "    # key is label | value is count\n",
    "    count_y_dict = {}\n",
    "    # key is word_i-1 , label_i | value is count\n",
    "    count_y_to_x_dict = {}\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        prev_word, prev_label = \"\", \"\"\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                word, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            else:\n",
    "                label = \"\"\n",
    "\n",
    "            # counting\n",
    "            if label == \"\" and prev_label != \"\":\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY, 0) + 1\n",
    "\n",
    "            elif label != \"\":\n",
    "                if prev_label == \"\":\n",
    "                    count_y_dict[START_STATE_KEY] = (\n",
    "                        count_y_dict.get(START_STATE_KEY, 0) + 1\n",
    "                    )\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label) + 1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "\n",
    "            # Counting unigram\n",
    "            if label != \"\" and prev_word != \"\":\n",
    "                count_y_to_x_dict[(label, prev_word)] = (\n",
    "                    count_y_to_x_dict.get((label, prev_word), 0) + 1\n",
    "                )\n",
    "\n",
    "            prev_word, prev_label = word, label\n",
    "\n",
    "    # Calculate unigram\n",
    "    for key, value in count_y_to_x_dict.items():  # Default is iterate keys()\n",
    "        label = key[0]\n",
    "        word = key[1]\n",
    "        string = f\"unigram_1: {label}+{word}\"\n",
    "\n",
    "        prob = value / count_y_dict.get(label)\n",
    "        emission_dict[string] = float(np.where(prob != 0, np.log(prob), LARGE_NEG))\n",
    "\n",
    "    print(\n",
    "        \"unigram_1 yi -> xi-1: \\n\",\n",
    "        list(emission_dict.items())[-10:],\n",
    "        len(emission_dict),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    return count_y_to_x_dict, emission_transition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_2_parameters(train_dir, emission_dict):\n",
    "    \"\"\"Calculates the transition parameters by count(y->x_i+1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: count(y->x_i+1)/count(y), keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels -> words_i+1)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict()\n",
    "    \"\"\"\n",
    "    # key is label | value is count\n",
    "    count_y_dict = {}\n",
    "    # key is word_i+1 , label_i | value is count\n",
    "    count_y_to_x_dict = {}\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        prev_word, prev_label = \"\", \"\"\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                word, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            else:\n",
    "                label = \"\"\n",
    "\n",
    "            # counting\n",
    "            if label == \"\" and prev_label != \"\":\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY, 0) + 1\n",
    "            elif label != \"\":\n",
    "                if prev_label == \"\":\n",
    "                    count_y_dict[START_STATE_KEY] = (\n",
    "                        count_y_dict.get(START_STATE_KEY, 0) + 1\n",
    "                    )\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label) + 1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "\n",
    "            if prev_label != \"\" and word != \"\":\n",
    "                count_y_to_x_dict[(prev_label, word)] = (\n",
    "                    count_y_to_x_dict.get((prev_label, word), 0) + 1\n",
    "                )\n",
    "\n",
    "            prev_word, prev_label = word, label\n",
    "\n",
    "    # Calculate unigram\n",
    "    for (label, word), value in count_y_to_x_dict.items():  # Default is iterate keys()\n",
    "        if prev_label != \"\" and word != \"\":\n",
    "            string = f\"unigram_2: {label}+{word}\"\n",
    "            prob = value / count_y_dict.get(label)\n",
    "            emission_dict[string] = float(np.where(prob != 0, np.log(prob), LARGE_NEG))\n",
    "\n",
    "    print(\n",
    "        \"unigram_2 yi -> x_i+1: \\n\",\n",
    "        list(emission_dict.items())[-10:],\n",
    "        len(emission_dict),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    return count_y_to_x_dict, emission_transition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_parameters(train_dir, emission_dict):\n",
    "    \"\"\"Calculates the transition parameters by count(y->x_i+1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: count(yi-1 -> yi -> xi)/count(y), keys are tuples of word and label ('B-neutral+O+B-neutral', -9.01768561)\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(label-1 -> labels -> words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('B-neutral+O+B-neutral', -9.01768561)\n",
    "    :rtype: dict()\n",
    "    \"\"\"\n",
    "    # key is label | value is count\n",
    "    count_y_dict = {}\n",
    "    # key is word_i+1 , label_i | value is count\n",
    "    count_y_to_y_to_x_dict = {}\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        prev_word, prev_label = \"\", \"\"\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                word, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            else:\n",
    "                label = \"\"\n",
    "\n",
    "            # counting\n",
    "            if label == \"\" and prev_label != \"\":\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY, 0) + 1\n",
    "            elif label != \"\":\n",
    "                if prev_label == \"\":\n",
    "                    count_y_dict[START_STATE_KEY] = (\n",
    "                        count_y_dict.get(START_STATE_KEY, 0) + 1\n",
    "                    )\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label) + 1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "\n",
    "            if prev_label != \"\" and word != \"\" and label != \"\":\n",
    "                count_y_to_y_to_x_dict[(prev_label, label, word)] = (\n",
    "                    count_y_to_y_to_x_dict.get((prev_label, label, word), 0) + 1\n",
    "                )\n",
    "\n",
    "            prev_label = label\n",
    "\n",
    "    # Calculate unigram\n",
    "    for key, value in count_y_to_y_to_x_dict.items():  # Default is iterate keys()\n",
    "        prev_label, label, word = key\n",
    "        if prev_label != \"\" and label != \"\" and word != \"\":\n",
    "            string = f\"bigram: {prev_label}+{label}+{word}\"\n",
    "            prob = value / count_y_dict.get(label)\n",
    "            emission_dict[string] = float(np.where(prob != 0, np.log(prob), LARGE_NEG))\n",
    "        prev_label = label\n",
    "\n",
    "    print(\n",
    "        \"bigram yi-1 -> yi -> xi: \\n\",\n",
    "        list(emission_dict.items())[-10:],\n",
    "        len(emission_dict),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    return count_y_to_y_to_x_dict, emission_transition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, emission_dict = MLE_emission_parameters(train_sentences)\n",
    "_, emission_dict = MLE_transition_parameters(train_dir, emission_dict)\n",
    "_, emission_dict = unigram_1_parameters(train_dir, emission_dict)\n",
    "_, emission_dict = unigram_2_parameters(train_dir, emission_dict)\n",
    "count_y_dict, emission_dict = bigram_parameters(train_dir, emission_dict)\n",
    "\n",
    "print(list(emission_dict.items())[:5])\n",
    "print(list(emission_dict.items())[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo_2(test_sentences, count_y_dict, emission_dict):\n",
    "    \"\"\"Decoding process that finds greedily finds the best possible labels from past MLE scores, saves file to output folder\n",
    "\n",
    "    :param test_sentences: our file tokenised sentences\n",
    "    :type test_sentences: list(tuple())\n",
    "\n",
    "    :param count_y_dict: Count of labels\n",
    "    :param count_y_dict: dict()\n",
    "\n",
    "    :param emission_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :param emission_dict: dict()\n",
    "    \"\"\"\n",
    "\n",
    "    pi = [{}]\n",
    "    path = {}\n",
    "    labels = count_y_dict.keys()\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    with open(\"output/dev.p5.out\", \"w\") as outfile:\n",
    "        for sentence in test_sentences:\n",
    "            # j = 0 (START)\n",
    "            for label in labels:\n",
    "                pi[0][label] = emission_dict.get(\n",
    "                    f\"transition: {'START'}+{label}\", LARGE_NEG\n",
    "                ) + emission_dict.get(f\"emission: {label}+{sentence[0][0]}\", LARGE_NEG)\n",
    "                path[label] = [label]\n",
    "            # j = 1 to N-1\n",
    "            for idx in range(1, len(sentence)):\n",
    "                pi.append({})\n",
    "                newpath = {}\n",
    "                for label_y in labels:\n",
    "                    (prob, label) = max(\n",
    "                        [\n",
    "                            (\n",
    "                                pi[idx - 1][prev_label]\n",
    "                                + emission_dict.get(\n",
    "                                    f\"transition: {prev_label}+{label_y}\", LARGE_NEG\n",
    "                                )\n",
    "                                + emission_dict.get(\n",
    "                                    f\"emission: {label_y}+{sentence[idx][0]}\", LARGE_NEG\n",
    "                                )\n",
    "                                + (\n",
    "                                    emission_dict.get(\n",
    "                                        f\"unigram_1: {label_y}+{sentence[idx-1][0]}\",\n",
    "                                        LARGE_NEG,\n",
    "                                    )\n",
    "                                )\n",
    "                                + (\n",
    "                                    emission_dict.get(\n",
    "                                        f\"unigram_2: {label_y}+{sentence[idx+1][0]}\",\n",
    "                                        LARGE_NEG,\n",
    "                                    )\n",
    "                                    if idx < len(sentence) - 1\n",
    "                                    else 0\n",
    "                                )\n",
    "                                + emission_dict.get(\n",
    "                                    f\"bigram: {prev_label}+{label_y}+{sentence[idx][0]}\",\n",
    "                                    LARGE_NEG,\n",
    "                                ),\n",
    "                                prev_label,\n",
    "                            )\n",
    "                            for prev_label in labels\n",
    "                        ]\n",
    "                    )\n",
    "                    pi[idx][label_y] = prob\n",
    "                    newpath[label_y] = path[label] + [label_y]\n",
    "                path = newpath\n",
    "            # j = N (STOP)\n",
    "            idx = len(sentence)\n",
    "            (prob, label) = max(\n",
    "                [\n",
    "                    (\n",
    "                        pi[idx - 1][label_y]\n",
    "                        + emission_dict.get(\n",
    "                            f\"transition: {label_y}+{'STOP'}\", LARGE_NEG\n",
    "                        ),\n",
    "                        label_y,\n",
    "                    )\n",
    "                    for label_y in labels\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # handle inconsistent length\n",
    "            if len(sentence) != len(path[label]):\n",
    "                print(len(sentence), len(path[label]))\n",
    "                raise Exception(\n",
    "                    \"{} has a different lenght with {}\".format(sentence, path[label])\n",
    "                )\n",
    "\n",
    "            # write to file\n",
    "            for i in range(len(sentence)):\n",
    "                line = f\"{sentence[i][0]} {path[label][i]}\\n\"\n",
    "                outfile.write(line)\n",
    "\n",
    "            outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_algo_2(test_sentences, count_y_dict, emission_dict)\n",
    "\n",
    "lines = evaluate_results(\"dataset/dev.out\", \"output/dev.p5.out\")\n",
    "res = conlleval.evaluate(lines)\n",
    "print(conlleval.report(res))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp-t8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2a4c03de4558ef5a607eb077c8ee8a6fae43eb49b3aff39fb8abe80b4d90c52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
