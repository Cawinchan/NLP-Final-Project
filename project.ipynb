{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import conlleval\n",
    "import copy \n",
    "\n",
    "train_dir = \"dataset/train\"\n",
    "test_dir = \"dataset/dev.in\"\n",
    "\n",
    "START_STATE_KEY = \"START\"\n",
    "STOP_STATE_KEY = \"STOP\"\n",
    "\n",
    "LARGE_NEG = -2**52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('All', 'O'), ('in', 'O'), ('all', 'O'), (',', 'O'), ('the', 'O'), ('food', 'B-positive'), ('was', 'O'), ('great', 'O'), ('(', 'O'), ('except', 'O'), ('for', 'O'), ('the', 'O'), ('dessserts', 'B-negative'), (')', 'O'), ('.', 'O')], [('I', 'O'), ('have', 'O'), ('NEVER', 'O'), ('been', 'O'), ('disappointed', 'O'), ('in', 'O'), ('the', 'O'), ('Red', 'B-positive'), ('Eye', 'I-positive'), ('.', 'O')], [('Great', 'O'), ('food', 'B-positive'), ('with', 'O'), ('an', 'O'), ('awesome', 'O'), ('atmosphere', 'B-positive'), ('!', 'O')], [('The', 'O'), ('sangria', 'B-positive'), ('was', 'O'), ('pretty', 'O'), ('tasty', 'O'), ('and', 'O'), ('good', 'O'), ('on', 'O'), ('a', 'O'), ('hot', 'O'), ('muggy', 'O'), ('day', 'O'), ('.', 'O')], [('Also', 'O'), (',', 'O'), ('waiters', 'B-negative'), ('try', 'O'), ('to', 'O'), ('push', 'O'), ('more', 'O'), ('food', 'O'), ('on', 'O'), ('you', 'O'), (',', 'O'), ('like', 'O'), ('suggest', 'O'), ('things', 'O'), ('as', 'O'), ('if', 'O'), ('they', 'O'), ('are', 'O'), ('complimentary', 'O'), ('when', 'O'), ('they', 'O'), ('actually', 'O'), ('cost', 'O'), ('$', 'O'), ('.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(file_path):  \n",
    "    data, lst = [], []\n",
    "    with open(file_path, 'r') as f: \n",
    "        for line in f:\n",
    "            if line== '\\n':\n",
    "                data.append(lst)\n",
    "                lst = []    \n",
    "            else:\n",
    "                lines = line.replace(\"\\n\",'').split(\" \")\n",
    "                lst.append(tuple(lines))\n",
    "    return data\n",
    "\n",
    "train_sentences = tokenize(train_dir)\n",
    "print(train_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "## 1i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MLE_emission_parameters(train_sentences):\n",
    "    ''' Calculates the emission parameters by count(y->x)/count(y)\n",
    "    \n",
    "    :param train_sentences: our train file tokenised sentences\n",
    "    :type train_sentences: list(tuple())\n",
    "\n",
    "    :return count_y_dict: Count of labels \n",
    "    :rtype: dict()\n",
    "\n",
    "    :return count_y_to_x_dict: Count of words and labels\n",
    "    :rtype: dict()\n",
    "\n",
    "    :param emission_dict: value of Count(labels->words)/Count(labels), keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict\n",
    "\n",
    "    '''\n",
    "\n",
    "    count_y_dict = {}\n",
    "    count_y_to_x_dict = {}\n",
    "    emission_dict = {}\n",
    "    word_bank = []\n",
    "\n",
    "    for sentence in train_sentences:\n",
    "        for x_y_pair in sentence:\n",
    "            word, label = x_y_pair\n",
    "            word_bank.append(word)\n",
    "            count_y_dict[label] = count_y_dict.get(label,0) + 1\n",
    "            count_y_to_x_dict[(label,word)] = count_y_to_x_dict.get((label,word),0) + 1\n",
    "\n",
    "    # Calculate our emission\n",
    "    for key, value in count_y_to_x_dict.items(): \n",
    "        label = key[0]\n",
    "        word = key[1]\n",
    "        string = f\"emission: {label}+{word}\" \n",
    "        prob =  value / count_y_dict.get(label)\n",
    "        emission_dict[string] = float(np.where(prob != 0, np.log(prob), 0))\n",
    "\n",
    "    # handle missing possible words and labels\n",
    "    unique_word_bank, labels = set(word_bank), count_y_dict.keys()\n",
    "    for label in labels:       \n",
    "        for word in unique_word_bank:\n",
    "            string = f\"emission: {label}+{word}\"\n",
    "            if string not in emission_dict:\n",
    "                emission_dict[string] = LARGE_NEG\n",
    "\n",
    "    return count_y_dict, count_y_to_x_dict, emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emission: O+All', -9.017685611042436), ('emission: O+in', -4.54034879656423), ('emission: O+all', -5.785564559424215), ('emission: O+,', -3.24728344904484), ('emission: O+the', -3.0916488692569097)]\n"
     ]
    }
   ],
   "source": [
    "count_y_dict, count_y_to_x_dict, emission_dict = MLE_emission_parameters(train_sentences)\n",
    "print(list(emission_dict.items())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  MLE_transition_parameters(train_dir, emission_dict):\n",
    "    ''' Calculates the transition parameters by count(y->y-1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: Count(y->x)/Count(y), keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict()\n",
    "    '''\n",
    "    count_y_dict = {}\n",
    "    count_y_to_y_dict = {}\n",
    "    prev_label = \"\"\n",
    "    emission_transition_dict = copy.deepcopy(emission_dict)\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                _, label = line.replace(\"\\n\",\"\").split(\" \")\n",
    "            else:\n",
    "                label = ''\n",
    "            if label == '' and prev_label != '':\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY) + 1 if count_y_dict.get(STOP_STATE_KEY) else 1\n",
    "            elif label !='':\n",
    "                if prev_label == '':\n",
    "                    count_y_dict[START_STATE_KEY] = count_y_dict.get(START_STATE_KEY) + 1 if count_y_dict.get(START_STATE_KEY) else 1\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label)+1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "            if prev_label == '' and label != '':\n",
    "                if (START_STATE_KEY, label) in count_y_to_y_dict:\n",
    "                    count_y_to_y_dict[(START_STATE_KEY, label)] = count_y_to_y_dict.get((START_STATE_KEY, label)) + 1\n",
    "                else:\n",
    "                    count_y_to_y_dict[(START_STATE_KEY, label)] = 1\n",
    "            elif label == '' and prev_label != '':\n",
    "                if (prev_label, STOP_STATE_KEY) in count_y_to_y_dict:\n",
    "                    count_y_to_y_dict[(prev_label, STOP_STATE_KEY)] = count_y_to_y_dict.get((prev_label, STOP_STATE_KEY)) + 1\n",
    "                else:\n",
    "                    count_y_to_y_dict[(prev_label, STOP_STATE_KEY)] = 1\n",
    "            elif label != '' and prev_label != '':\n",
    "                if (prev_label, label) in count_y_to_y_dict:\n",
    "                    count_y_to_y_dict[(prev_label, label)] = count_y_to_y_dict.get((prev_label, label)) + 1\n",
    "                else:\n",
    "                    count_y_to_y_dict[(prev_label, label)] = 1\n",
    "            prev_label = label\n",
    "\n",
    "    # Calculate our transition\n",
    "    for key, value in count_y_to_y_dict.items(): # Default is iterate keys()\n",
    "        prev_label = key[0]\n",
    "        label = key[1]\n",
    "        string = f\"transition: {prev_label}+{label}\" \n",
    "        prob =  value / count_y_dict.get(prev_label)\n",
    "        emission_transition_dict[string] = float(np.where(prob != 0, np.log(prob), 0))\n",
    "    # print(\"MLE: \\n\",list(emission_transition_dict.items()), len(emission_transition_dict) ,\"\\n\")\n",
    "\n",
    "    labels = count_y_dict.keys()\n",
    "    for label in labels:\n",
    "        for prev_label in labels:\n",
    "            string = f\"transition: {prev_label}+{label}\" \n",
    "            if string not in emission_transition_dict:\n",
    "                emission_transition_dict[string] = LARGE_NEG\n",
    "\n",
    "    return count_y_to_y_dict, emission_transition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emission: O+All', -9.017685611042436), ('emission: O+in', -4.54034879656423), ('emission: O+all', -5.785564559424215), ('emission: O+,', -3.24728344904484), ('emission: O+the', -3.0916488692569097)]\n",
      "[('transition: B-positive+I-negative', -4503599627370496), ('transition: STOP+I-negative', -4503599627370496), ('transition: I-positive+I-negative', -4503599627370496), ('transition: B-neutral+I-negative', -4503599627370496), ('transition: I-neutral+I-negative', -4503599627370496)]\n"
     ]
    }
   ],
   "source": [
    "count_y_to_y_dict, emission_transition_dict = MLE_transition_parameters(train_dir, emission_dict)\n",
    "print(list(emission_transition_dict.items())[:5])\n",
    "print(list(emission_transition_dict.items())[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sentence, emission_transition_dict):\n",
    "    ''' Calculates the score with of a given pair based on emission and transmission features\n",
    "    \n",
    "    :param sentences: our  file tokenised sentences\n",
    "    :type sentences: list(tuple())\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :type emission_transition_dict: dict()\n",
    "\n",
    "    :param score: our emission score + transition score for sentence\n",
    "    :type sentences: float\n",
    "    '''\n",
    "    score = 0\n",
    "    emission_score = 0 \n",
    "    transition_score = 0\n",
    "    x_seq = [x[0] for x in sentence]\n",
    "    y_seq = [START_STATE_KEY]+[y[1] for y in sentence]+[STOP_STATE_KEY]\n",
    "    \n",
    "    for i in range(len(x_seq)):\n",
    "        label = y_seq[i+1]\n",
    "        word = x_seq[i]\n",
    "        key = f\"emission: {label}+{word}\" \n",
    "        emission_score += emission_transition_dict[key]\n",
    "    for j in range(1, len(y_seq)):\n",
    "        prev_label = y_seq[j-1]\n",
    "        label = y_seq[j]\n",
    "        key = f\"transition: {prev_label}+{label}\" \n",
    "        transition_score += emission_transition_dict[key]\n",
    "    score = emission_score + transition_score\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-85.52845366888094"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(train_sentences[0],emission_transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Loved',), ('it',)], [('The',), ('music',), ('playing',), ('was',), ('very',), ('hip',), (',',), ('20-30',), ('something',), ('pop',), ('music',), (',',), ('but',), ('the',), ('subwoofer',), ('to',), ('the',), ('sound',), ('system',), ('was',), ('located',), ('under',), ('my',), ('seat',), (',',), ('which',), ('became',), ('annoying',), ('midway',), ('through',), ('dinner',), ('.',)], [('This',), ('place',), ('has',), ('ruined',), ('me',), ('for',), ('neighborhood',), ('sushi',), ('.',)], [('I',), ('have',), ('never',), ('eaten',), ('in',), ('the',), ('restaurant',), (',',), ('however',), (',',), ('upon',), ('reading',), ('the',), ('reviews',), ('I',), ('got',), ('take',), ('out',), ('last',), ('week',), ('.',)], [('It',), (\"isn't\",), ('the',), ('cheapest',), ('sushi',), ('but',), ('has',), ('been',), ('worth',), ('it',), ('every',), ('time',), ('.',)]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = tokenize(test_dir)\n",
    "print(test_sentences[:5])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(test_sentences, count_y_dict, emission_transition_dict, output_name):\n",
    "    ''' Decoding process that finds globally finds the best possible labels from past MLE scores, saves file to output folder\n",
    "    \n",
    "    :param test_sentences: our file tokenised sentences\n",
    "    :type test_sentences: list(tuple())\n",
    "\n",
    "    :param count_y_dict: Count of labels \n",
    "    :param count_y_dict: dict()\n",
    "\n",
    "    :param emission_transition_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :param emission_transition_dict: dict()\n",
    "    '''\n",
    "    \n",
    "    pi = [{}]\n",
    "    path = {}\n",
    "    labels = count_y_dict.keys()\n",
    "    os.makedirs('output',exist_ok=True)\n",
    "\n",
    "    with open(f'output/{output_name}', \"w\") as outfile:\n",
    "        for sentence in test_sentences:\n",
    "            # j = 0 (START)\n",
    "            for label in labels:\n",
    "                pi[0][label] = emission_transition_dict.get(f\"transition: {START_STATE_KEY}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[0][0]}\",LARGE_NEG)\n",
    "                path[label] = [label]\n",
    "            # j = 1 to N-1\n",
    "            for idx in range(1,len(sentence)):\n",
    "                pi.append({})\n",
    "                newpath = {}\n",
    "                for label_y in labels:\n",
    "                    (prob, label) = max([(pi[idx-1][prev_label] + emission_transition_dict.get(f\"transition: {prev_label}+{label_y}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label_y}+{sentence[idx][0]}\",LARGE_NEG), prev_label) \n",
    "                                    for prev_label in labels])\n",
    "                    pi[idx][label_y] = prob\n",
    "                    newpath[label_y] = path[label] + [label_y]\n",
    "                path = newpath\n",
    "            # j = N (STOP)\n",
    "            idx = len(sentence)\n",
    "            (prob, label) = max([(pi[idx-1][label_y] + emission_transition_dict.get(f\"transition: {label_y}+{STOP_STATE_KEY}\", LARGE_NEG), label_y) for label_y in labels])\n",
    "            \n",
    "            # handle inconsistent length\n",
    "            if len(sentence) != len(path[label]):\n",
    "                print(len(sentence),len(path[label]))\n",
    "                raise Exception(\"{} has a different lenght with {}\".format(sentence, path[label]))\n",
    "            \n",
    "            # write to file\n",
    "            for i in range(len(sentence)):\n",
    "                line = \"{} {}\\n\".format(sentence[i][0], path[label][i])\n",
    "                outfile.write(line)\n",
    "                \n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_algo(test_sentences, count_y_dict, emission_transition_dict, output_name='dev.p2.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of dev.p2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3809 tokens with 210 phrases; found: 132 phrases; correct: 63.\n",
      "accuracy:  93.23%; precision:  47.73%; recall:  30.00%; FB1:  36.84\n",
      "         negative: precision:  35.29%; recall:   9.23%; FB1:  14.63  17\n",
      "          neutral: precision:  20.00%; recall:  12.50%; FB1:  15.38  5\n",
      "         positive: precision:  50.91%; recall:  40.88%; FB1:  45.34  110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_dir = 'output/dev.p2.out'\n",
    "truth_dir = 'dataset/dev.out'\n",
    "\n",
    "def evaluate_results(truth_dir,prediction_dir):\n",
    "    predictions = []\n",
    "    prediction_sentences = tokenize(prediction_dir)\n",
    "    for sentence in prediction_sentences:\n",
    "        for word_pair in sentence:\n",
    "            predictions.append(word_pair[1])     \n",
    "    lines = \"\"\"\"\"\"\n",
    "    idx = 0\n",
    "    with open(truth_dir, \"r\", encoding=\"utf8\") as tstr:\n",
    "        for line in tstr:\n",
    "            if len(line) > 1:\n",
    "                newline = line.replace(\"\\n\",f\" {predictions[idx]}\\n\")\n",
    "                lines += newline\n",
    "                idx += 1\n",
    "            else:\n",
    "                lines += \"\\n\"\n",
    "    return lines.splitlines()\n",
    "\n",
    "lines = evaluate_results(truth_dir,prediction_dir)\n",
    "res = conlleval.evaluate(lines)\n",
    "print(conlleval.report(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "## 3i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'O': -9.079345204990318,\n",
       "   'B-positive': -4503599627370499.0,\n",
       "   'B-negative': -4503599627370501.0,\n",
       "   'I-positive': -9007199254740992,\n",
       "   'B-neutral': -4503599627370501.0,\n",
       "   'I-neutral': -9007199254740992,\n",
       "   'I-negative': -9007199254740992},\n",
       "  {'O': -13.766622979615455,\n",
       "   'B-positive': -4503599627370508.0,\n",
       "   'B-negative': -4503599627370509.0,\n",
       "   'I-positive': -4503599627370506.0,\n",
       "   'B-neutral': -4503599627370511.0,\n",
       "   'I-neutral': -9007199254741000.0,\n",
       "   'I-negative': -4503599627370507.0},\n",
       "  {'O': -19.699116517100578,\n",
       "   'B-positive': -23.953109923204384,\n",
       "   'B-negative': -23.93728208854489,\n",
       "   'I-positive': -9007199254741004.0,\n",
       "   'B-neutral': -4503599627370516.0,\n",
       "   'I-neutral': -9007199254741006.0,\n",
       "   'I-negative': -9007199254741004.0},\n",
       "  {'O': -23.068893163028275,\n",
       "   'B-positive': -4503599627370519.0,\n",
       "   'B-negative': -4503599627370520.0,\n",
       "   'I-positive': -30.46462428231426,\n",
       "   'B-neutral': -4503599627370522.0,\n",
       "   'I-neutral': -9007199254741012.0,\n",
       "   'I-negative': -4503599627370522.0},\n",
       "  {'O': -26.307057565319838,\n",
       "   'B-positive': -4503599627370522.0,\n",
       "   'B-negative': -4503599627370523.0,\n",
       "   'I-positive': -36.10781617712779,\n",
       "   'B-neutral': -4503599627370525.0,\n",
       "   'I-neutral': -9007199254741016.0,\n",
       "   'I-negative': -4503599627370523.0},\n",
       "  {'O': -32.599955203725884,\n",
       "   'B-positive': -31.618347185707613,\n",
       "   'B-negative': -32.86679876160505,\n",
       "   'I-positive': -40.18239215402747,\n",
       "   'B-neutral': -34.10786696720044,\n",
       "   'I-neutral': -4503599627370525.0,\n",
       "   'I-negative': -4503599627370527.0},\n",
       "  {'O': -35.36430708731581,\n",
       "   'B-positive': -4503599627370532.0,\n",
       "   'B-negative': -4503599627370533.0,\n",
       "   'I-positive': -4503599627370529.0,\n",
       "   'B-neutral': -4503599627370535.0,\n",
       "   'I-neutral': -4503599627370532.0,\n",
       "   'I-negative': -4503599627370530.0},\n",
       "  {'O': -40.60365344424098,\n",
       "   'B-positive': -4503599627370534.0,\n",
       "   'B-negative': -4503599627370536.0,\n",
       "   'I-positive': -9007199254741026.0,\n",
       "   'B-neutral': -4503599627370537.0,\n",
       "   'I-neutral': -9007199254741028.0,\n",
       "   'I-negative': -9007199254741028.0},\n",
       "  {'O': -46.510171495322844,\n",
       "   'B-positive': -4503599627370540.0,\n",
       "   'B-negative': -4503599627370541.0,\n",
       "   'I-positive': -4503599627370539.0,\n",
       "   'B-neutral': -4503599627370543.0,\n",
       "   'I-neutral': -9007199254741032.0,\n",
       "   'I-negative': -9007199254741032.0},\n",
       "  {'O': -55.38710401197441,\n",
       "   'B-positive': -4503599627370546.0,\n",
       "   'B-negative': -4503599627370547.0,\n",
       "   'I-positive': -9007199254741036.0,\n",
       "   'B-neutral': -4503599627370548.0,\n",
       "   'I-neutral': -9007199254741040.0,\n",
       "   'I-negative': -9007199254741040.0},\n",
       "  {'O': -60.00135665158466,\n",
       "   'B-positive': -4503599627370554.0,\n",
       "   'B-negative': -4503599627370556.0,\n",
       "   'I-positive': -4503599627370553.0,\n",
       "   'B-neutral': -4503599627370557.0,\n",
       "   'I-neutral': -9007199254741046.0,\n",
       "   'I-negative': -4503599627370554.0},\n",
       "  {'O': -63.23993449890247,\n",
       "   'B-positive': -4503599627370559.0,\n",
       "   'B-negative': -4503599627370560.0,\n",
       "   'I-positive': -4503599627370559.0,\n",
       "   'B-neutral': -4503599627370562.0,\n",
       "   'I-neutral': -9007199254741052.0,\n",
       "   'I-negative': -4503599627370559.0},\n",
       "  {'O': -4503599627370559.0,\n",
       "   'B-positive': -4503599627370562.0,\n",
       "   'B-negative': -73.41059360783191,\n",
       "   'I-positive': -9007199254741054.0,\n",
       "   'B-neutral': -4503599627370565.0,\n",
       "   'I-neutral': -9007199254741056.0,\n",
       "   'I-negative': -9007199254741056.0},\n",
       "  {'O': -79.33052533092805,\n",
       "   'B-positive': -9007199254741058.0,\n",
       "   'B-negative': -9007199254741060.0,\n",
       "   'I-positive': -4503599627370567.0,\n",
       "   'B-neutral': -9007199254741060.0,\n",
       "   'I-neutral': -9007199254741064.0,\n",
       "   'I-negative': -4503599627370571.0},\n",
       "  {'O': -82.2752126601316,\n",
       "   'B-positive': -4503599627370578.0,\n",
       "   'B-negative': -4503599627370580.0,\n",
       "   'I-positive': -9007199254741064.0,\n",
       "   'B-neutral': -4503599627370581.0,\n",
       "   'I-neutral': -9007199254741072.0,\n",
       "   'I-negative': -9007199254741068.0}],\n",
       " -84.8718181557256)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logSumExp(a):\n",
    "    max = np.max(a)\n",
    "    sumOfExp = np.exp(a - max).sum()\n",
    "    return max + np.log(sumOfExp)\n",
    "\n",
    "def forward_algorithm(sentence, count_y_dict, emission_transition_dict):    \n",
    "    pi = [{}]\n",
    "    labels = count_y_dict.keys()\n",
    "    # j = 0 (START)\n",
    "    for label in labels:\n",
    "        pi[0][label] = emission_transition_dict.get(f\"transition: {START_STATE_KEY}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[0][0]}\",LARGE_NEG)\n",
    "\n",
    "    # j = 1 to N-1\n",
    "    for idx in range(1,len(sentence)):\n",
    "        pi.append({})\n",
    "\n",
    "        for label in labels:\n",
    "            log_a = []\n",
    "            for prev_label in labels:\n",
    "                log_a.append(pi[idx-1][prev_label] + emission_transition_dict.get(f\"transition: {prev_label}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[idx][0]}\",LARGE_NEG))\n",
    "            pi[idx][label] = logSumExp(log_a)\n",
    "            \n",
    "    # j = N (STOP)\n",
    "    idx = len(sentence)\n",
    "    log_a = []\n",
    "    for label in labels:\n",
    "        log_a.append(pi[idx-1][label] + emission_transition_dict.get(f\"transition: {label}+{STOP_STATE_KEY}\", LARGE_NEG))\n",
    "    return pi, logSumExp(log_a)\n",
    "\n",
    "forward_algorithm(train_sentences[0], count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2050.74053383538"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(sentences, count_y_dict, emission_transition_dict):\n",
    "    loss = 0\n",
    "    for sent in sentences:\n",
    "        loss+= score(sent, emission_transition_dict)\n",
    "        _, update = forward_algorithm(sent, count_y_dict, emission_transition_dict)\n",
    "        loss-= update\n",
    "    return (-1)*loss\n",
    "loss_fn(train_sentences, count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_algorithm(sentence, count_y_dict, emission_transition_dict):    \n",
    "    pi = [{} for i in range(len(sentence))]\n",
    "    labels = count_y_dict.keys()\n",
    "\n",
    "    # j = N (STOP)\n",
    "    for label in labels:\n",
    "        pi[len(sentence)-1][label] = emission_transition_dict.get(f\"transition: {label}+{STOP_STATE_KEY}\", LARGE_NEG)\n",
    "\n",
    "    # j = N-1 to 1 \n",
    "    for idx in range(len(sentence)-1,0,-1):\n",
    "        for label in labels:\n",
    "            log_b = []\n",
    "            for next_label in labels:\n",
    "                log_b.append(pi[idx][next_label] + emission_transition_dict.get(f\"transition: {label}+{next_label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {next_label}+{sentence[idx][0]}\",LARGE_NEG))\n",
    "            pi[idx-1][label] = logSumExp(log_b)\n",
    "\n",
    "    # j = 0 (START)\n",
    "    log_b = []\n",
    "    for label in labels:\n",
    "        log_b.append(pi[0][label] + emission_transition_dict.get(f\"transition: {START_STATE_KEY}+{label}\",LARGE_NEG) + emission_transition_dict.get(f\"emission: {label}+{sentence[0][0]}\",LARGE_NEG))\n",
    "    \n",
    "    return pi, logSumExp(log_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'O': -75.79247295073526,\n",
       "   'B-positive': -75.96013210037187,\n",
       "   'B-negative': -75.65394924920902,\n",
       "   'I-positive': -76.11520236719656,\n",
       "   'B-neutral': -75.87847553035473,\n",
       "   'I-neutral': -76.29947044008102,\n",
       "   'I-negative': -75.64641026599291},\n",
       "  {'O': -71.10519517611013,\n",
       "   'B-positive': -71.34020585812414,\n",
       "   'B-negative': -71.21765957710079,\n",
       "   'I-positive': -71.52518608374271,\n",
       "   'B-neutral': -71.21614051255226,\n",
       "   'I-neutral': -71.63713542227855,\n",
       "   'I-negative': -71.49666270104414},\n",
       "  {'O': -65.19764439544767,\n",
       "   'B-positive': -65.36383891214948,\n",
       "   'B-negative': -65.28516603961566,\n",
       "   'I-positive': -65.51792969991837,\n",
       "   'B-neutral': -65.28364697506713,\n",
       "   'I-neutral': -65.70464188479342,\n",
       "   'I-negative': -65.56416916355901},\n",
       "  {'O': -61.80343196834192,\n",
       "   'B-positive': -61.890165966505435,\n",
       "   'B-negative': -61.831706821012546,\n",
       "   'I-positive': -61.994494944318916,\n",
       "   'B-neutral': -61.889434547961386,\n",
       "   'I-neutral': -62.310429457687675,\n",
       "   'I-negative': -62.020553032988104},\n",
       "  {'O': -58.564854121024105,\n",
       "   'B-positive': -58.32274068137175,\n",
       "   'B-negative': -59.55602438373126,\n",
       "   'I-positive': -58.04127044988653,\n",
       "   'B-neutral': -58.830965146599674,\n",
       "   'I-neutral': -58.0427976934884,\n",
       "   'I-negative': -59.33749196269587},\n",
       "  {'O': -53.674913383749015,\n",
       "   'B-positive': -53.88525725696903,\n",
       "   'B-negative': -53.76243502791701,\n",
       "   'I-positive': -54.06996153455894,\n",
       "   'B-neutral': -53.76091596336849,\n",
       "   'I-neutral': -54.18191087309478,\n",
       "   'I-negative': -54.041438151860376},\n",
       "  {'O': -49.50751106840979,\n",
       "   'B-positive': -49.717854941629795,\n",
       "   'B-negative': -49.59503271257778,\n",
       "   'I-positive': -49.90255921921971,\n",
       "   'B-neutral': -49.59351364802926,\n",
       "   'I-neutral': -50.01450855775555,\n",
       "   'I-negative': -49.874035836521145},\n",
       "  {'O': -44.268164711484616,\n",
       "   'B-positive': -43.61902437617982,\n",
       "   'B-negative': -44.35568635565261,\n",
       "   'I-positive': -43.45656805803591,\n",
       "   'B-neutral': -44.35416729110408,\n",
       "   'I-neutral': -44.77516220083037,\n",
       "   'I-negative': -44.63468947959597},\n",
       "  {'O': -38.361646660402755,\n",
       "   'B-positive': -38.57199053362276,\n",
       "   'B-negative': -38.44916830457075,\n",
       "   'I-positive': -38.756694811212675,\n",
       "   'B-neutral': -38.44764924002222,\n",
       "   'I-neutral': -38.86864414974851,\n",
       "   'I-negative': -38.72817142851411},\n",
       "  {'O': -29.48471414375119,\n",
       "   'B-positive': -29.61520927208044,\n",
       "   'B-negative': -29.458136952015625,\n",
       "   'I-positive': -29.746145856385503,\n",
       "   'B-neutral': -29.57071672337066,\n",
       "   'I-neutral': -29.991711633096948,\n",
       "   'I-negative': -29.574300149101838},\n",
       "  {'O': -24.870461504140945,\n",
       "   'B-positive': -25.08080537736095,\n",
       "   'B-negative': -24.95798314830894,\n",
       "   'I-positive': -25.265509654950865,\n",
       "   'B-neutral': -24.956464083760412,\n",
       "   'I-neutral': -25.3774589934867,\n",
       "   'I-negative': -25.2369862722523},\n",
       "  {'O': -21.631883656823128,\n",
       "   'B-positive': -4503599627370507.0,\n",
       "   'B-negative': -4503599627370508.0,\n",
       "   'I-positive': -4503599627370507.0,\n",
       "   'B-neutral': -4503599627370508.0,\n",
       "   'I-neutral': -4503599627370508.0,\n",
       "   'I-negative': -4503599627370508.0},\n",
       "  {'O': -11.373702903725697,\n",
       "   'B-positive': -10.830678494159349,\n",
       "   'B-negative': -11.461224547893693,\n",
       "   'I-positive': -10.692627318251152,\n",
       "   'B-neutral': -11.459705483345164,\n",
       "   'I-neutral': -11.880700393071455,\n",
       "   'I-negative': -11.74022767183705},\n",
       "  {'O': -5.541292824797559,\n",
       "   'B-positive': -5.7516366980175615,\n",
       "   'B-negative': -5.628814468965554,\n",
       "   'I-positive': -5.936340975607477,\n",
       "   'B-neutral': -5.6272954044170245,\n",
       "   'I-neutral': -6.048290314143316,\n",
       "   'I-negative': -5.907817592908913},\n",
       "  {'O': -2.5966054955940074,\n",
       "   'B-positive': -4.252688120309395,\n",
       "   'B-negative': -4.836281906951478,\n",
       "   'I-positive': -4.564348191467836,\n",
       "   'B-neutral': -3.245193133185574,\n",
       "   'I-neutral': -4503599627370496,\n",
       "   'I-negative': -4503599627370496}],\n",
       " -84.87181815572558)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_algorithm(train_sentences[0], count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transition: START+O': 1.0000000000000284,\n",
       " 'emission: O+All': 1.0000000000000284,\n",
       " 'transition: START+B-positive': 0.0,\n",
       " 'emission: B-positive+All': 0.0,\n",
       " 'transition: START+B-negative': 0.0,\n",
       " 'emission: B-negative+All': 0.0,\n",
       " 'transition: START+I-positive': 0.0,\n",
       " 'emission: I-positive+All': 0.0,\n",
       " 'transition: START+B-neutral': 0.0,\n",
       " 'emission: B-neutral+All': 0.0,\n",
       " 'transition: START+I-neutral': 0.0,\n",
       " 'emission: I-neutral+All': 0.0,\n",
       " 'transition: START+I-negative': 0.0,\n",
       " 'emission: I-negative+All': 0.0,\n",
       " 'transition: O+O': 10.441907229302245,\n",
       " 'transition: B-positive+O': 0.5428700135957362,\n",
       " 'transition: B-negative+O': 1.185388749994202,\n",
       " 'transition: I-positive+O': 0.0005068471541609425,\n",
       " 'transition: B-neutral+O': 0.04993841289642202,\n",
       " 'transition: I-neutral+O': 0.0,\n",
       " 'transition: I-negative+O': 0.0,\n",
       " 'emission: O+in': 1.0000000000000284,\n",
       " 'transition: O+B-positive': 0.5433768607498974,\n",
       " 'transition: B-positive+B-positive': 0.0,\n",
       " 'transition: B-negative+B-positive': 0.0,\n",
       " 'transition: I-positive+B-positive': 0.0,\n",
       " 'transition: B-neutral+B-positive': 0.0,\n",
       " 'transition: I-neutral+B-positive': 0.0,\n",
       " 'transition: I-negative+B-positive': 0.0,\n",
       " 'emission: B-positive+in': 0.0,\n",
       " 'transition: O+B-negative': 1.1853887499942022,\n",
       " 'transition: B-positive+B-negative': 0.0,\n",
       " 'transition: B-negative+B-negative': 0.0,\n",
       " 'transition: I-positive+B-negative': 0.0,\n",
       " 'transition: B-neutral+B-negative': 0.0,\n",
       " 'transition: I-neutral+B-negative': 0.0,\n",
       " 'transition: I-negative+B-negative': 0.0,\n",
       " 'emission: B-negative+in': 0.0,\n",
       " 'transition: O+I-positive': 0.0,\n",
       " 'transition: B-positive+I-positive': 0.0005068471541609574,\n",
       " 'transition: B-negative+I-positive': 0.0,\n",
       " 'transition: I-positive+I-positive': 0.00017787626272221284,\n",
       " 'transition: B-neutral+I-positive': 0.0,\n",
       " 'transition: I-neutral+I-positive': 0.0,\n",
       " 'transition: I-negative+I-positive': 0.0,\n",
       " 'emission: I-positive+in': 0.0,\n",
       " 'transition: O+B-neutral': 0.04993841289642202,\n",
       " 'transition: B-positive+B-neutral': 0.0,\n",
       " 'transition: B-negative+B-neutral': 0.0,\n",
       " 'transition: I-positive+B-neutral': 0.0,\n",
       " 'transition: B-neutral+B-neutral': 0.0,\n",
       " 'transition: I-neutral+B-neutral': 0.0,\n",
       " 'transition: I-negative+B-neutral': 0.0,\n",
       " 'emission: B-neutral+in': 0.0,\n",
       " 'transition: O+I-neutral': 0.0,\n",
       " 'transition: B-positive+I-neutral': 0.0,\n",
       " 'transition: B-negative+I-neutral': 0.0,\n",
       " 'transition: I-positive+I-neutral': 0.0,\n",
       " 'transition: B-neutral+I-neutral': 0.0,\n",
       " 'transition: I-neutral+I-neutral': 0.0,\n",
       " 'transition: I-negative+I-neutral': 0.0,\n",
       " 'emission: I-neutral+in': 0.0,\n",
       " 'transition: O+I-negative': 0.0,\n",
       " 'transition: B-positive+I-negative': 0.0,\n",
       " 'transition: B-negative+I-negative': 0.0,\n",
       " 'transition: I-positive+I-negative': 0.0,\n",
       " 'transition: B-neutral+I-negative': 0.0,\n",
       " 'transition: I-neutral+I-negative': 0.0,\n",
       " 'transition: I-negative+I-negative': 0.0,\n",
       " 'emission: I-negative+in': 0.0,\n",
       " 'emission: O+all': 0.9753657434645776,\n",
       " 'emission: B-positive+all': 0.011735572330503501,\n",
       " 'emission: B-negative+all': 0.012898684204926262,\n",
       " 'emission: I-positive+all': 0.0,\n",
       " 'emission: B-neutral+all': 0.0,\n",
       " 'emission: I-neutral+all': 0.0,\n",
       " 'emission: I-negative+all': 0.0,\n",
       " 'emission: O+,': 0.9994931528458538,\n",
       " 'emission: B-positive+,': 0.0,\n",
       " 'emission: B-negative+,': 0.0,\n",
       " 'emission: I-positive+,': 0.0005068471541609502,\n",
       " 'emission: B-neutral+,': 0.0,\n",
       " 'emission: I-neutral+,': 0.0,\n",
       " 'emission: I-negative+,': 0.0,\n",
       " 'emission: O+the': 1.9999064737555226,\n",
       " 'emission: B-positive+the': 0.0,\n",
       " 'emission: B-negative+the': 0.0,\n",
       " 'emission: I-positive+the': 9.352624449320648e-05,\n",
       " 'emission: B-neutral+the': 0.0,\n",
       " 'emission: I-neutral+the': 0.0,\n",
       " 'emission: I-negative+the': 0.0,\n",
       " 'emission: O+food': 0.24584588287669223,\n",
       " 'emission: B-positive+food': 0.5316412884193862,\n",
       " 'emission: B-negative+food': 0.17249006578927584,\n",
       " 'emission: I-positive+food': 8.435001822900635e-05,\n",
       " 'emission: B-neutral+food': 0.04993841289642202,\n",
       " 'emission: I-neutral+food': 0.0,\n",
       " 'emission: I-negative+food': 0.0,\n",
       " 'emission: O+was': 1.0,\n",
       " 'emission: B-positive+was': 0.0,\n",
       " 'emission: B-negative+was': 0.0,\n",
       " 'emission: I-positive+was': 0.0,\n",
       " 'emission: B-neutral+was': 0.0,\n",
       " 'emission: I-neutral+was': 0.0,\n",
       " 'emission: I-negative+was': 0.0,\n",
       " 'emission: O+great': 1.0,\n",
       " 'emission: B-positive+great': 0.0,\n",
       " 'emission: B-negative+great': 0.0,\n",
       " 'emission: I-positive+great': 0.0,\n",
       " 'emission: B-neutral+great': 0.0,\n",
       " 'emission: I-neutral+great': 0.0,\n",
       " 'emission: I-negative+great': 0.0,\n",
       " 'emission: O+(': 1.0,\n",
       " 'emission: B-positive+(': 0.0,\n",
       " 'emission: B-negative+(': 0.0,\n",
       " 'emission: I-positive+(': 0.0,\n",
       " 'emission: B-neutral+(': 0.0,\n",
       " 'emission: I-neutral+(': 0.0,\n",
       " 'emission: I-negative+(': 0.0,\n",
       " 'emission: O+except': 1.0,\n",
       " 'emission: B-positive+except': 0.0,\n",
       " 'emission: B-negative+except': 0.0,\n",
       " 'emission: I-positive+except': 0.0,\n",
       " 'emission: B-neutral+except': 0.0,\n",
       " 'emission: I-neutral+except': 0.0,\n",
       " 'emission: I-negative+except': 0.0,\n",
       " 'emission: O+for': 1.0,\n",
       " 'emission: B-positive+for': 0.0,\n",
       " 'emission: B-negative+for': 0.0,\n",
       " 'emission: I-positive+for': 0.0,\n",
       " 'emission: B-neutral+for': 0.0,\n",
       " 'emission: I-neutral+for': 0.0,\n",
       " 'emission: I-negative+for': 0.0,\n",
       " 'emission: O+dessserts': 0.0,\n",
       " 'emission: B-positive+dessserts': 0.0,\n",
       " 'emission: B-negative+dessserts': 1.0,\n",
       " 'emission: I-positive+dessserts': 0.0,\n",
       " 'emission: B-neutral+dessserts': 0.0,\n",
       " 'emission: I-neutral+dessserts': 0.0,\n",
       " 'emission: I-negative+dessserts': 0.0,\n",
       " 'emission: O+)': 1.0,\n",
       " 'emission: B-positive+)': 0.0,\n",
       " 'emission: B-negative+)': 0.0,\n",
       " 'emission: I-positive+)': 0.0,\n",
       " 'emission: B-neutral+)': 0.0,\n",
       " 'emission: I-neutral+)': 0.0,\n",
       " 'emission: I-negative+)': 0.0,\n",
       " 'emission: O+.': 1.0,\n",
       " 'emission: B-positive+.': 0.0,\n",
       " 'emission: B-negative+.': 0.0,\n",
       " 'emission: I-positive+.': 0.0,\n",
       " 'emission: B-neutral+.': 0.0,\n",
       " 'emission: I-neutral+.': 0.0,\n",
       " 'emission: I-negative+.': 0.0,\n",
       " 'transition: O+STOP': 1.0,\n",
       " 'transition: B-positive+STOP': 0.0,\n",
       " 'transition: B-negative+STOP': 0.0,\n",
       " 'transition: I-positive+STOP': 0.0,\n",
       " 'transition: B-neutral+STOP': 0.0,\n",
       " 'transition: I-neutral+STOP': 0.0,\n",
       " 'transition: I-negative+STOP': 0.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_backward_algorithm(sentence, count_y_dict, emission_transition_dict):\n",
    "    feature_expectation = {}\n",
    "    labels = count_y_dict.keys()\n",
    "    fwd_pi, fwd_score = forward_algorithm(sentence, count_y_dict, emission_transition_dict)\n",
    "    bkd_pi, bkd_score = backward_algorithm(sentence, count_y_dict, emission_transition_dict)\n",
    "        \n",
    "    # idx = 1\n",
    "    for label in labels:\n",
    "        string_transition = f\"transition: {START_STATE_KEY}+{label}\"\n",
    "        string_emission = f\"emission: {label}+{sentence[0][0]}\" \n",
    "\n",
    "        # update transition features\n",
    "        update = bkd_pi[0][label] \\\n",
    "                + emission_transition_dict[string_transition] \\\n",
    "                + emission_transition_dict[string_emission] \\\n",
    "                - fwd_score\n",
    "         \n",
    "        feature_expectation[string_transition] = feature_expectation.get(string_transition,0) + np.exp(update)\n",
    "        \n",
    "        # update emission features\n",
    "        update = fwd_pi[0][label] + bkd_pi[0][label] - fwd_score\n",
    "        feature_expectation[string_emission] = feature_expectation.get(string_emission,0) + np.exp(update)\n",
    "        \n",
    "    # idx = 2 to N-1\n",
    "    for idx in range(1,len(sentence)):\n",
    "        for label in labels:\n",
    "            string_emission = f\"emission: {label}+{sentence[idx][0]}\" \n",
    "\n",
    "            # update transition features\n",
    "            for prev_label in labels:\n",
    "                string_transition = f\"transition: {prev_label}+{label}\" \n",
    "                update = fwd_pi[idx-1][prev_label] \\\n",
    "                        + bkd_pi[idx][label] \\\n",
    "                        + emission_transition_dict[string_transition] \\\n",
    "                        + emission_transition_dict[string_emission] \\\n",
    "                        - fwd_score\n",
    "                feature_expectation[string_transition] = feature_expectation.get(string_transition,0) + np.exp(update)\n",
    "\n",
    "            # update emission features\n",
    "            update = fwd_pi[idx][label] + bkd_pi[idx][label] - fwd_score      \n",
    "            feature_expectation[string_emission] = feature_expectation.get(string_emission,0) + np.exp(update)\n",
    "                \n",
    "    # idx = N (STOP)\n",
    "    idx = len(sentence)\n",
    "    for label in labels:\n",
    "        # update transition features\n",
    "        string_transition = f\"transition: {label}+{STOP_STATE_KEY}\" \n",
    "        update = fwd_pi[idx-1][label] + emission_transition_dict[string_transition] - fwd_score\n",
    "        feature_expectation[string_transition] = feature_expectation.get(string_transition,0) + np.exp(update)\n",
    "        \n",
    "    return feature_expectation\n",
    "                \n",
    "forward_backward_algorithm(train_sentences[0], count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sentences, count_y_dict, emission_transition_dict):\n",
    "    features = {k:0 for k,v in emission_transition_dict.items()}\n",
    "    for sent in sentences:\n",
    "        expect = forward_backward_algorithm(sent, count_y_dict, emission_transition_dict)\n",
    "        for k,v in expect.items():\n",
    "            features[k] += v\n",
    "    return features\n",
    "\n",
    "features = get_features(train_sentences, count_y_dict, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fn(emission_transition_dict):\n",
    "    index_map = {}\n",
    "    for idx, value in enumerate(emission_transition_dict): \n",
    "        if len(value.split(\" \")[1].split(\"+\")) > 2:\n",
    "            first = str(value.split(\" \")[1].split(\"+\")[0])\n",
    "            second = '+'\n",
    "        else:\n",
    "            first = str(value.split(\" \")[1].split(\"+\")[0])\n",
    "            second = str(value.split(\" \")[1].split(\"+\")[1])\n",
    "        index_map[idx] = (first,second)\n",
    "    return index_map\n",
    "\n",
    "index_map = mapping_fn(emission_transition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.26325641e-14, -3.14611815e+00, -2.90336197e-01, ...,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_grad(sentences, count_y_dict, features, index_map, emission_transition_dict):\n",
    "    labels = count_y_dict.keys()\n",
    "    counter = 0 \n",
    "    grad_lst = np.zeros(len(emission_transition_dict),)\n",
    "    for i in range(len(features)):\n",
    "        # print(index_map[i],count_y_to_x_dict.keys())\n",
    "        if index_map[i] in count_y_to_x_dict.keys():\n",
    "            string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            grad_lst[i] += (features[string] - count_y_to_x_dict[index_map[i]])\n",
    "        elif index_map[i] in count_y_to_y_dict.keys():\n",
    "            string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            grad_lst[i] += (features[string] - count_y_to_y_dict[index_map[i]])\n",
    "        else:\n",
    "            try:\n",
    "                string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "                grad_lst[i] += (features[string])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "                grad_lst[i] += (features[string])\n",
    "            except:\n",
    "                pass\n",
    "    return grad_lst\n",
    "\n",
    "compute_grad(train_sentences, count_y_dict, features, index_map, emission_transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4\n",
    "## 4i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_with_reg(w, sentences, count_y_dict, emission_transition_dict, n = 0.1):\n",
    "    loss = loss_fn(sentences, count_y_dict, emission_transition_dict)\n",
    "    loss += n*sum(w1*w1 for w1 in w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_with_reg(w, sentences, count_y_dict, features, index_map, emission_transition_dict, n = 0.1):\n",
    "    grad_lst = compute_grad(sentences, count_y_dict, features, index_map, emission_transition_dict)\n",
    "    for i in range(len(w)):\n",
    "        grad_lst[i] += w[i]*2*n\n",
    "    return grad_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 52775.02915252912\n",
      "time taken: 17.978718757629395 total time: 18.051718711853027\n",
      "loss: 31978.422560595387\n",
      "time taken: 18.113350868225098 total time: 36.192068576812744\n",
      "loss: 19007.65777640147\n",
      "time taken: 17.230767965316772 total time: 53.42583632469177\n",
      "loss: 19007.65777640147\n",
      "time taken: 16.818676471710205 total time: 70.24848580360413\n",
      "Loss:19007.6578\n",
      "loss: 15909.840926992103\n",
      "time taken: 16.53831124305725 total time: 86.79179739952087\n",
      "loss: 24692.805625405286\n",
      "time taken: 16.614581823349 total time: 103.41038990020752\n",
      "loss: 12068.697074799862\n",
      "time taken: 16.62595796585083 total time: 120.0413224697113\n",
      "loss: 12068.697074799862\n",
      "time taken: 16.613444328308105 total time: 136.65776705741882\n",
      "Loss:12068.6971\n",
      "loss: 42879.194680706445\n",
      "time taken: 16.625876426696777 total time: 153.28864431381226\n",
      "loss: 11393.865776589319\n",
      "time taken: 16.497926473617554 total time: 169.79057216644287\n",
      "loss: 11393.865776589319\n",
      "time taken: 16.60111403465271 total time: 186.3956868648529\n",
      "Loss:11393.8658\n",
      "loss: 10788.15567690617\n",
      "time taken: 16.60189127922058 total time: 203.0015799999237\n",
      "loss: 10788.15567690617\n",
      "time taken: 17.085956573486328 total time: 220.09153628349304\n",
      "Loss:10788.1557\n",
      "loss: 10197.25111554726\n",
      "time taken: 16.628634214401245 total time: 236.72517228126526\n",
      "loss: 10197.25111554726\n",
      "time taken: 16.60098910331726 total time: 253.32912611961365\n",
      "Loss:10197.2511\n",
      "loss: 9635.861130429555\n",
      "time taken: 16.619665384292603 total time: 269.95379161834717\n",
      "loss: 9635.861130429555\n",
      "time taken: 16.644445657730103 total time: 286.6012647151947\n",
      "Loss:9635.8611\n",
      "loss: 8861.897153827294\n",
      "time taken: 16.830179452896118 total time: 303.43744683265686\n",
      "loss: 8861.897153827294\n",
      "time taken: 16.622847318649292 total time: 320.0643198490143\n",
      "Loss:8861.8972\n",
      "loss: 8284.076747442528\n",
      "time taken: 16.62486171722412 total time: 336.694149017334\n",
      "loss: 8284.076747442528\n",
      "time taken: 16.59043049812317 total time: 353.2885808944702\n",
      "Loss:8284.0767\n",
      "loss: 8078.550089725975\n",
      "time taken: 16.590187788009644 total time: 369.885746717453\n",
      "loss: 8078.550089725975\n",
      "time taken: 16.5749409198761 total time: 386.4637129306793\n",
      "Loss:8078.5501\n",
      "loss: 7887.952221264266\n",
      "time taken: 16.54378628730774 total time: 403.0124661922455\n",
      "loss: 7887.952221264266\n",
      "time taken: 16.708269357681274 total time: 419.7247359752655\n",
      "Loss:7887.9522\n",
      "loss: 7669.0498952222615\n",
      "time taken: 16.537726879119873 total time: 436.2694640159607\n",
      "loss: 7669.0498952222615\n",
      "time taken: 16.644826412200928 total time: 452.91732358932495\n",
      "Loss:7669.0499\n",
      "loss: 7304.06179889031\n",
      "time taken: 16.64111089706421 total time: 469.56443548202515\n",
      "loss: 7304.06179889031\n",
      "time taken: 16.589702606201172 total time: 486.1581389904022\n",
      "Loss:7304.0618\n",
      "loss: 6878.975796460469\n",
      "time taken: 16.545958518981934 total time: 502.7100603580475\n",
      "loss: 6878.975796460469\n",
      "time taken: 16.674083948135376 total time: 519.3881425857544\n",
      "Loss:6878.9758\n",
      "loss: 6513.320422939939\n",
      "time taken: 16.638832092285156 total time: 536.0339457988739\n",
      "loss: 6513.320422939939\n",
      "time taken: 16.643459796905518 total time: 552.6809771060944\n",
      "Loss:6513.3204\n",
      "loss: 6348.989212527013\n",
      "time taken: 16.638678073883057 total time: 569.3256134986877\n",
      "loss: 6348.989212527013\n",
      "time taken: 16.60756492614746 total time: 585.9371800422668\n",
      "Loss:6348.9892\n",
      "loss: 6077.388244170685\n",
      "time taken: 16.68115258216858 total time: 602.6243343353271\n",
      "loss: 6077.388244170685\n",
      "time taken: 17.22388982772827 total time: 619.8522250652313\n",
      "Loss:6077.3882\n",
      "loss: 5959.001193162048\n",
      "time taken: 16.48738431930542 total time: 636.346287727356\n",
      "loss: 5959.001193162048\n",
      "time taken: 16.389054775238037 total time: 652.7393724918365\n",
      "Loss:5959.0012\n",
      "loss: 5657.876857057016\n",
      "time taken: 16.28726887702942 total time: 669.0326397418976\n",
      "loss: 5657.876857057016\n",
      "time taken: 16.436951398849487 total time: 685.4725902080536\n",
      "Loss:5657.8769\n",
      "loss: 5434.752094427957\n",
      "time taken: 16.318097591400146 total time: 701.7966890335083\n",
      "loss: 5434.752094427957\n",
      "time taken: 16.36106252670288 total time: 718.1617512702942\n",
      "Loss:5434.7521\n",
      "loss: 5743.379462404202\n",
      "time taken: 16.333727598190308 total time: 734.5014815330505\n",
      "loss: 5381.266331201675\n",
      "time taken: 16.36314368247986 total time: 750.8686261177063\n",
      "loss: 5381.266331201675\n",
      "time taken: 16.37564444541931 total time: 767.2482380867004\n",
      "Loss:5381.2663\n",
      "loss: 5333.562398984037\n",
      "time taken: 16.349736213684082 total time: 783.602972984314\n",
      "loss: 5333.562398984037\n",
      "time taken: 16.535582304000854 total time: 800.1425807476044\n",
      "Loss:5333.5624\n",
      "loss: 5302.8242387021355\n",
      "time taken: 17.89575219154358 total time: 818.0453040599823\n",
      "loss: 5302.8242387021355\n",
      "time taken: 17.808614253997803 total time: 835.8579208850861\n",
      "Loss:5302.8242\n",
      "loss: 5214.821254215742\n",
      "time taken: 17.9650776386261 total time: 853.8292245864868\n",
      "loss: 5214.821254215742\n",
      "time taken: 16.959732055664062 total time: 870.792956829071\n",
      "Loss:5214.8213\n",
      "loss: 5109.091252869708\n",
      "time taken: 17.6808443069458 total time: 888.4797749519348\n",
      "loss: 5109.091252869708\n",
      "time taken: 17.473228454589844 total time: 905.9579770565033\n",
      "Loss:5109.0913\n",
      "loss: 4954.769118242758\n",
      "time taken: 17.667571544647217 total time: 923.6315166950226\n",
      "loss: 4954.769118242758\n",
      "time taken: 18.016346216201782 total time: 941.6518633365631\n",
      "Loss:4954.7691\n",
      "loss: 4904.418471304348\n",
      "time taken: 17.524449825286865 total time: 959.182285785675\n",
      "loss: 4904.418471304348\n",
      "time taken: 17.493335485458374 total time: 976.6795926094055\n",
      "Loss:4904.4185\n",
      "loss: 4758.848208067353\n",
      "time taken: 17.150562286376953 total time: 993.8361518383026\n",
      "loss: 4758.848208067353\n",
      "time taken: 17.85442352294922 total time: 1011.6945745944977\n",
      "Loss:4758.8482\n",
      "loss: 4651.419723344664\n",
      "time taken: 17.142289400100708 total time: 1028.842863559723\n",
      "loss: 4651.419723344664\n",
      "time taken: 17.079845428466797 total time: 1045.9267106056213\n",
      "Loss:4651.4197\n",
      "loss: 4557.89595774837\n",
      "time taken: 17.435286045074463 total time: 1063.3679673671722\n",
      "loss: 4557.89595774837\n",
      "time taken: 17.7206974029541 total time: 1081.0926632881165\n",
      "Loss:4557.8960\n",
      "loss: 4489.40239521727\n",
      "time taken: 17.577993154525757 total time: 1098.6766583919525\n",
      "loss: 4489.40239521727\n",
      "time taken: 18.121572494506836 total time: 1116.8022291660309\n",
      "Loss:4489.4024\n",
      "loss: 4391.653249200081\n",
      "time taken: 17.33444905281067 total time: 1134.1426787376404\n",
      "loss: 4391.653249200081\n",
      "time taken: 17.335862398147583 total time: 1151.4825389385223\n",
      "Loss:4391.6532\n",
      "loss: 4344.1130161477495\n",
      "time taken: 16.273280382156372 total time: 1167.7618191242218\n",
      "loss: 4344.1130161477495\n",
      "time taken: 18.005548238754272 total time: 1185.770367383957\n",
      "Loss:4344.1130\n",
      "loss: 4210.479992014277\n",
      "time taken: 17.673574686050415 total time: 1203.4504790306091\n",
      "loss: 4210.479992014277\n",
      "time taken: 17.16085982322693 total time: 1220.6143424510956\n",
      "Loss:4210.4800\n",
      "loss: 4105.660003330031\n",
      "time taken: 17.408498525619507 total time: 1238.0288400650024\n",
      "loss: 4105.660003330031\n",
      "time taken: 17.521984100341797 total time: 1255.5548276901245\n",
      "Loss:4105.6600\n",
      "loss: 4075.4750173385783\n",
      "time taken: 17.691861867904663 total time: 1273.2526881694794\n",
      "loss: 4075.4750173385783\n",
      "time taken: 17.93489384651184 total time: 1291.1915826797485\n",
      "Loss:4075.4750\n",
      "loss: 4021.341228125867\n",
      "time taken: 18.92495608329773 total time: 1310.12153673172\n",
      "loss: 4021.341228125867\n",
      "time taken: 18.017289876937866 total time: 1328.1428263187408\n",
      "Loss:4021.3412\n",
      "loss: 4128.563512919455\n",
      "time taken: 17.753011226654053 total time: 1345.9018414020538\n",
      "loss: 3972.4630536024843\n",
      "time taken: 18.18455410003662 total time: 1364.0903975963593\n",
      "loss: 3972.4630536024843\n",
      "time taken: 18.343806266784668 total time: 1382.4372026920319\n",
      "Loss:3972.4631\n",
      "loss: 3984.8897487603012\n",
      "time taken: 19.260965585708618 total time: 1401.7046530246735\n",
      "loss: 3928.744925690231\n",
      "time taken: 18.951504707336426 total time: 1420.6601302623749\n",
      "loss: 3928.744925690231\n",
      "time taken: 18.66208004951477 total time: 1439.327178478241\n",
      "Loss:3928.7449\n",
      "loss: 3883.9008003131125\n",
      "time taken: 18.699676990509033 total time: 1458.0328559875488\n",
      "loss: 3883.9008003131125\n",
      "time taken: 17.547747373580933 total time: 1475.5855784416199\n",
      "Loss:3883.9008\n",
      "loss: 3868.378691175448\n",
      "time taken: 18.83545184135437 total time: 1494.4265427589417\n",
      "loss: 3868.378691175448\n",
      "time taken: 18.17342448234558 total time: 1512.604968070984\n",
      "Loss:3868.3787\n",
      "loss: 3817.616628901539\n",
      "time taken: 17.905985832214355 total time: 1530.5169517993927\n",
      "loss: 3817.616628901539\n",
      "time taken: 17.335237741470337 total time: 1547.856188774109\n",
      "Loss:3817.6166\n",
      "loss: 3794.772537462322\n",
      "time taken: 16.96452522277832 total time: 1564.8256769180298\n",
      "loss: 3794.772537462322\n",
      "time taken: 17.465285062789917 total time: 1582.2959604263306\n",
      "Loss:3794.7725\n",
      "loss: 3737.794456893808\n",
      "time taken: 17.78863024711609 total time: 1600.0905900001526\n",
      "loss: 3737.794456893808\n",
      "time taken: 18.916321992874146 total time: 1619.011886358261\n",
      "Loss:3737.7945\n",
      "loss: 3677.2696821612526\n",
      "time taken: 17.659732818603516 total time: 1636.6766157150269\n",
      "loss: 3677.2696821612526\n",
      "time taken: 17.58705759048462 total time: 1654.2667014598846\n",
      "Loss:3677.2697\n",
      "loss: 3698.270810700038\n",
      "time taken: 18.55267882347107 total time: 1672.8253817558289\n",
      "loss: 3645.2906873179436\n",
      "time taken: 17.03842782974243 total time: 1689.8677797317505\n",
      "loss: 3645.2906873179436\n",
      "time taken: 17.327402591705322 total time: 1707.199203968048\n",
      "Loss:3645.2907\n",
      "loss: 3629.341480729705\n",
      "time taken: 18.741477012634277 total time: 1725.9470841884613\n",
      "loss: 3629.341480729705\n",
      "time taken: 17.57262897491455 total time: 1743.5237164497375\n",
      "Loss:3629.3415\n",
      "loss: 3590.4551955513252\n",
      "time taken: 16.94676446914673 total time: 1760.476454257965\n",
      "loss: 3590.4551955513252\n",
      "time taken: 18.09673810005188 total time: 1778.5772321224213\n",
      "Loss:3590.4552\n",
      "loss: 3524.230616546091\n",
      "time taken: 17.138217449188232 total time: 1795.7224493026733\n",
      "loss: 3524.230616546091\n",
      "time taken: 17.418748140335083 total time: 1813.1451964378357\n",
      "Loss:3524.2306\n",
      "loss: 3554.798747316445\n",
      "time taken: 17.425973653793335 total time: 1830.5791726112366\n",
      "loss: 3504.9608096835673\n",
      "time taken: 16.96469521522522 total time: 1847.5478699207306\n",
      "loss: 3504.9608096835673\n",
      "time taken: 17.256277084350586 total time: 1864.807143688202\n",
      "Loss:3504.9608\n",
      "loss: 3481.1155723787915\n",
      "time taken: 18.75818133354187 total time: 1883.5713241100311\n",
      "loss: 3481.1155723787915\n",
      "time taken: 18.832395553588867 total time: 1902.407747745514\n",
      "Loss:3481.1156\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "total_start = time.time()\n",
    "\n",
    "def callbackF(w):\n",
    "    '''\n",
    "    This function will only be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    '''\n",
    "    loss = get_loss_grad(w)[0]\n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w):\n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    Returns:\n",
    "    loss: loss, float\n",
    "    grads: gradients, numpy array\n",
    "    '''\n",
    "    start = time.time()\n",
    "    new_emission_transition_dict = {}\n",
    "    for i in range(len(index_map.keys())):\n",
    "        if i < len(emission_dict):\n",
    "            string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            new_emission_transition_dict[string] = w[i]\n",
    "        else:\n",
    "            string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "            new_emission_transition_dict[string] = w[i]\n",
    "    features = get_features(train_sentences, count_y_dict, new_emission_transition_dict)\n",
    "\n",
    "    loss = loss_with_reg(w, train_sentences, count_y_dict, new_emission_transition_dict, n = 0.1)\n",
    "    print('loss: '+ str(loss))\n",
    "    \n",
    "    grad_lst = grad_with_reg(w, train_sentences, count_y_dict, features, index_map, new_emission_transition_dict, n = 0.1)\n",
    "    grads = np.asarray(list(grad_lst)) \n",
    "    \n",
    "    print('time taken: '+ str(time.time()-start) +' total time: '+ str(time.time()-total_start))\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(len(index_map),)\n",
    "results = fmin_l_bfgs_b(get_loss_grad, init_w, pgtol=0.01, maxiter=50, callback=callbackF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emission_transition_dict = {}\n",
    "for i in range(len(index_map.keys())): #27899\n",
    "    if i< len(emission_dict): #27818\n",
    "        string = f'emission: {index_map[i][0]}+{index_map[i][1]}'\n",
    "        new_emission_transition_dict[string] = results[0][i]\n",
    "    else:\n",
    "        string = f'transition: {index_map[i][0]}+{index_map[i][1]}'\n",
    "        new_emission_transition_dict[string] = results[0][i]\n",
    "\n",
    "viterbi_algo(test_sentences, count_y_dict, new_emission_transition_dict, 'dev.p4.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3809 tokens with 210 phrases; found: 153 phrases; correct: 66.\n",
      "accuracy:  92.23%; precision:  43.14%; recall:  31.43%; FB1:  36.36\n",
      "         negative: precision:  50.00%; recall:   9.23%; FB1:  15.58  12\n",
      "          neutral: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "         positive: precision:  43.48%; recall:  43.80%; FB1:  43.64  138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_dir = 'output/dev.p4.out'\n",
    "truth_dir = 'dataset/dev.out'\n",
    "\n",
    "lines = evaluate_results(truth_dir,prediction_dir)\n",
    "res = conlleval.evaluate(lines)\n",
    "print(conlleval.report(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_1_parameters(train_dir, emission_dict):\n",
    "    \"\"\"Calculates the transition parameters by count(y->x_i-1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: count(y->x_i-1)/count(y), keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels->words_i-1)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict()\n",
    "    \"\"\"\n",
    "    # key is label | value is count\n",
    "    count_y_dict = {}\n",
    "    # key is word_i-1 , label_i | value is count\n",
    "    count_y_to_x_dict = {}\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        prev_word, prev_label = \"\", \"\"\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                word, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            else:\n",
    "                label = \"\"\n",
    "\n",
    "            # counting\n",
    "            if label == \"\" and prev_label != \"\":\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY, 0) + 1\n",
    "\n",
    "            elif label != \"\":\n",
    "                if prev_label == \"\":\n",
    "                    count_y_dict[START_STATE_KEY] = (\n",
    "                        count_y_dict.get(START_STATE_KEY, 0) + 1\n",
    "                    )\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label) + 1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "\n",
    "            # Counting unigram\n",
    "            if label != \"\" and prev_word != \"\":\n",
    "                count_y_to_x_dict[(label, prev_word)] = (\n",
    "                    count_y_to_x_dict.get((label, prev_word), 0) + 1\n",
    "                )\n",
    "\n",
    "            prev_word, prev_label = word, label\n",
    "\n",
    "    # Calculate unigram\n",
    "    for key, value in count_y_to_x_dict.items():  # Default is iterate keys()\n",
    "        label = key[0]\n",
    "        word = key[1]\n",
    "        string = f\"unigram_1: {label}+{word}\"\n",
    "\n",
    "        prob = value / count_y_dict.get(label)\n",
    "        emission_dict[string] = float(np.where(prob != 0, np.log(prob), LARGE_NEG))\n",
    "\n",
    "    print(\n",
    "        \"unigram_1 yi -> xi-1: \\n\",\n",
    "        list(emission_dict.items())[-10:],\n",
    "        len(emission_dict),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    return count_y_to_x_dict, emission_transition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_2_parameters(train_dir, emission_dict):\n",
    "    \"\"\"Calculates the transition parameters by count(y->x_i+1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: count(y->x_i+1)/count(y), keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(labels -> words_i+1)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('unigram_1: O+All', -9.01768561), value MLE\n",
    "    :rtype: dict()\n",
    "    \"\"\"\n",
    "    # key is label | value is count\n",
    "    count_y_dict = {}\n",
    "    # key is word_i+1 , label_i | value is count\n",
    "    count_y_to_x_dict = {}\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        prev_word, prev_label = \"\", \"\"\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                word, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            else:\n",
    "                label = \"\"\n",
    "\n",
    "            # counting\n",
    "            if label == \"\" and prev_label != \"\":\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY, 0) + 1\n",
    "            elif label != \"\":\n",
    "                if prev_label == \"\":\n",
    "                    count_y_dict[START_STATE_KEY] = (\n",
    "                        count_y_dict.get(START_STATE_KEY, 0) + 1\n",
    "                    )\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label) + 1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "\n",
    "            if prev_label != \"\" and word != \"\":\n",
    "                count_y_to_x_dict[(prev_label, word)] = (\n",
    "                    count_y_to_x_dict.get((prev_label, word), 0) + 1\n",
    "                )\n",
    "\n",
    "            prev_word, prev_label = word, label\n",
    "\n",
    "    # Calculate unigram\n",
    "    for (label, word), value in count_y_to_x_dict.items():  # Default is iterate keys()\n",
    "        if prev_label != \"\" and word != \"\":\n",
    "            string = f\"unigram_2: {label}+{word}\"\n",
    "            prob = value / count_y_dict.get(label)\n",
    "            emission_dict[string] = float(np.where(prob != 0, np.log(prob), LARGE_NEG))\n",
    "\n",
    "    print(\n",
    "        \"unigram_2 yi -> x_i+1: \\n\",\n",
    "        list(emission_dict.items())[-10:],\n",
    "        len(emission_dict),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    return count_y_to_x_dict, emission_transition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_parameters(train_dir, emission_dict):\n",
    "    \"\"\"Calculates the transition parameters by count(y->x_i+1)/count(y)\n",
    "\n",
    "    :param train_dir: our train file\n",
    "    :type train_sentences: str\n",
    "\n",
    "    :param emission_dict: count(yi-1 -> yi -> xi)/count(y), keys are tuples of word and label ('B-neutral+O+B-neutral', -9.01768561)\n",
    "    :type emission_dict: dict()\n",
    "\n",
    "    :return count_y_to_y_dict: Count of labels and previous label\n",
    "    :rtype: dict()\n",
    "\n",
    "    :return emission_transition_dict: value of Count(label-1 -> labels -> words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('B-neutral+O+B-neutral', -9.01768561)\n",
    "    :rtype: dict()\n",
    "    \"\"\"\n",
    "    # key is label | value is count\n",
    "    count_y_dict = {}\n",
    "    # key is word_i+1 , label_i | value is count\n",
    "    count_y_to_y_to_x_dict = {}\n",
    "\n",
    "    with open(train_dir, \"r\", encoding=\"utf8\") as f:\n",
    "        prev_word, prev_label = \"\", \"\"\n",
    "        for line in f:\n",
    "            # Parse each line\n",
    "            if len(line.split(\" \")) == 2:\n",
    "                word, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            else:\n",
    "                label = \"\"\n",
    "\n",
    "            # counting\n",
    "            if label == \"\" and prev_label != \"\":\n",
    "                count_y_dict[STOP_STATE_KEY] = count_y_dict.get(STOP_STATE_KEY, 0) + 1\n",
    "            elif label != \"\":\n",
    "                if prev_label == \"\":\n",
    "                    count_y_dict[START_STATE_KEY] = (\n",
    "                        count_y_dict.get(START_STATE_KEY, 0) + 1\n",
    "                    )\n",
    "                if label in count_y_dict:\n",
    "                    count_y_dict[label] = count_y_dict.get(label) + 1\n",
    "                else:\n",
    "                    count_y_dict[label] = 1\n",
    "\n",
    "            if prev_label != \"\" and word != \"\" and label != \"\":\n",
    "                count_y_to_y_to_x_dict[(prev_label, label, word)] = (\n",
    "                    count_y_to_y_to_x_dict.get((prev_label, label, word), 0) + 1\n",
    "                )\n",
    "\n",
    "            prev_label = label\n",
    "\n",
    "    # Calculate unigram\n",
    "    for key, value in count_y_to_y_to_x_dict.items():  # Default is iterate keys()\n",
    "        prev_label, label, word = key\n",
    "        if prev_label != \"\" and label != \"\" and word != \"\":\n",
    "            string = f\"bigram: {prev_label}+{label}+{word}\"\n",
    "            prob = value / count_y_dict.get(label)\n",
    "            emission_dict[string] = float(np.where(prob != 0, np.log(prob), LARGE_NEG))\n",
    "        prev_label = label\n",
    "\n",
    "    print(\n",
    "        \"bigram yi-1 -> yi -> xi: \\n\",\n",
    "        list(emission_dict.items())[-10:],\n",
    "        len(emission_dict),\n",
    "        \"\\n\",\n",
    "    )\n",
    "    emission_transition_dict = emission_dict\n",
    "\n",
    "    return count_y_to_y_to_x_dict, emission_transition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_1 yi -> xi-1: \n",
      " [('unigram_1: O+combination', -10.116297899710545), ('unigram_1: B-positive+super-fresh', -7.0859014643656115), ('unigram_1: O+unusual', -10.116297899710545), ('unigram_1: B-neutral+biggest', -4.343805421853684), ('unigram_1: O+adequate', -10.116297899710545), (\"unigram_1: O+Mom's\", -10.116297899710545), ('unigram_1: O+leaving', -10.116297899710545), (\"unigram_1: O+we're\", -10.116297899710545), ('unigram_1: O+hurry', -10.116297899710545), ('unigram_1: O+originally', -10.116297899710545)] 32447 \n",
      "\n",
      "unigram_2 yi -> x_i+1: \n",
      " [('unigram_1: O+combination', -10.116297899710545), ('unigram_1: B-positive+super-fresh', -7.0859014643656115), ('unigram_1: O+unusual', -10.116297899710545), ('unigram_1: B-neutral+biggest', -4.343805421853684), ('unigram_1: O+adequate', -10.116297899710545), (\"unigram_1: O+Mom's\", -10.116297899710545), ('unigram_1: O+leaving', -10.116297899710545), (\"unigram_1: O+we're\", -10.116297899710545), ('unigram_1: O+hurry', -10.116297899710545), ('unigram_1: O+originally', -10.116297899710545)] 32447 \n",
      "\n",
      "bigram yi-1 -> yi -> xi: \n",
      " [('bigram: O+O+combination', -10.116297899710545), ('bigram: O+O+super-fresh', -10.116297899710545), ('bigram: O+O+unusual', -10.116297899710545), ('bigram: O+O+biggest', -10.116297899710545), ('bigram: O+O+adequate', -10.116297899710545), (\"bigram: O+O+Mom's\", -10.116297899710545), ('bigram: O+O+leaving', -10.116297899710545), (\"bigram: O+O+we're\", -10.116297899710545), ('bigram: O+O+hurry', -10.116297899710545), ('bigram: O+O+originally', -10.116297899710545)] 37068 \n",
      "\n",
      "[('emission: O+All', -9.017685611042436), ('emission: O+in', -4.54034879656423), ('emission: O+all', -5.785564559424215), ('emission: O+,', -3.24728344904484), ('emission: O+the', -3.0916488692569097)]\n",
      "[(\"bigram: O+O+Mom's\", -10.116297899710545), ('bigram: O+O+leaving', -10.116297899710545), (\"bigram: O+O+we're\", -10.116297899710545), ('bigram: O+O+hurry', -10.116297899710545), ('bigram: O+O+originally', -10.116297899710545)]\n"
     ]
    }
   ],
   "source": [
    "_, _, emission_dict = MLE_emission_parameters(train_sentences)\n",
    "_, emission_dict = MLE_transition_parameters(train_dir, emission_dict)\n",
    "_, emission_dict = unigram_1_parameters(train_dir, emission_dict)\n",
    "_, emission_dict = unigram_2_parameters(train_dir, emission_dict)\n",
    "count_y_dict, emission_dict = bigram_parameters(train_dir, emission_dict)\n",
    "\n",
    "print(list(emission_dict.items())[:5])\n",
    "print(list(emission_dict.items())[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo_2(test_sentences, count_y_dict, emission_dict):\n",
    "    \"\"\"Decoding process that finds greedily finds the best possible labels from past MLE scores, saves file to output folder\n",
    "\n",
    "    :param test_sentences: our file tokenised sentences\n",
    "    :type test_sentences: list(tuple())\n",
    "\n",
    "    :param count_y_dict: Count of labels\n",
    "    :param count_y_dict: dict()\n",
    "\n",
    "    :param emission_dict: value of Count(labels->words)/Count(labels) for emission and Count(prev_labels->labels)/Count(labels) for transmission, keys are tuples of word and label ('emission: O+All', -9.01768561), value MLE\n",
    "    :param emission_dict: dict()\n",
    "    \"\"\"\n",
    "\n",
    "    pi = [{}]\n",
    "    path = {}\n",
    "    labels = count_y_dict.keys()\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    with open(\"output/dev.p5.out\", \"w\") as outfile:\n",
    "        for sentence in test_sentences:\n",
    "            # j = 0 (START)\n",
    "            for label in labels:\n",
    "                pi[0][label] = emission_dict.get(\n",
    "                    f\"transition: {'START'}+{label}\", LARGE_NEG\n",
    "                ) + emission_dict.get(f\"emission: {label}+{sentence[0][0]}\", LARGE_NEG)\n",
    "                path[label] = [label]\n",
    "            # j = 1 to N-1\n",
    "            for idx in range(1, len(sentence)):\n",
    "                pi.append({})\n",
    "                newpath = {}\n",
    "                for label_y in labels:\n",
    "                    (prob, label) = max(\n",
    "                        [\n",
    "                            (\n",
    "                                pi[idx - 1][prev_label]\n",
    "                                + emission_dict.get(\n",
    "                                    f\"transition: {prev_label}+{label_y}\", LARGE_NEG\n",
    "                                )\n",
    "                                + emission_dict.get(\n",
    "                                    f\"emission: {label_y}+{sentence[idx][0]}\", LARGE_NEG\n",
    "                                )\n",
    "                                + (\n",
    "                                    emission_dict.get(\n",
    "                                        f\"unigram_1: {label_y}+{sentence[idx-1][0]}\",\n",
    "                                        LARGE_NEG,\n",
    "                                    )\n",
    "                                )\n",
    "                                + (\n",
    "                                    emission_dict.get(\n",
    "                                        f\"unigram_2: {label_y}+{sentence[idx+1][0]}\",\n",
    "                                        LARGE_NEG,\n",
    "                                    )\n",
    "                                    if idx < len(sentence) - 1\n",
    "                                    else 0\n",
    "                                )\n",
    "                                + emission_dict.get(\n",
    "                                    f\"bigram: {prev_label}+{label_y}+{sentence[idx][0]}\",\n",
    "                                    LARGE_NEG,\n",
    "                                ),\n",
    "                                prev_label,\n",
    "                            )\n",
    "                            for prev_label in labels\n",
    "                        ]\n",
    "                    )\n",
    "                    pi[idx][label_y] = prob\n",
    "                    newpath[label_y] = path[label] + [label_y]\n",
    "                path = newpath\n",
    "            # j = N (STOP)\n",
    "            idx = len(sentence)\n",
    "            (prob, label) = max(\n",
    "                [\n",
    "                    (\n",
    "                        pi[idx - 1][label_y]\n",
    "                        + emission_dict.get(\n",
    "                            f\"transition: {label_y}+{'STOP'}\", LARGE_NEG\n",
    "                        ),\n",
    "                        label_y,\n",
    "                    )\n",
    "                    for label_y in labels\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # handle inconsistent length\n",
    "            if len(sentence) != len(path[label]):\n",
    "                print(len(sentence), len(path[label]))\n",
    "                raise Exception(\n",
    "                    \"{} has a different lenght with {}\".format(sentence, path[label])\n",
    "                )\n",
    "\n",
    "            # write to file\n",
    "            for i in range(len(sentence)):\n",
    "                line = f\"{sentence[i][0]} {path[label][i]}\\n\"\n",
    "                outfile.write(line)\n",
    "\n",
    "            outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-2f746ad0a298>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mviterbi_algo_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_y_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memission_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset/dev.out\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output/dev.p5.out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconlleval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconlleval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-dd2eec5fa8f3>\u001b[0m in \u001b[0;36mviterbi_algo_2\u001b[1;34m(test_sentences, count_y_dict, emission_dict)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                 \u001b[0mprev_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                             )\n\u001b[1;32m---> 62\u001b[1;33m                             \u001b[1;32mfor\u001b[0m \u001b[0mprev_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                         ]\n\u001b[0;32m     64\u001b[0m                     )\n",
      "\u001b[1;32m<ipython-input-54-dd2eec5fa8f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                 \u001b[0mprev_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                             )\n\u001b[1;32m---> 62\u001b[1;33m                             \u001b[1;32mfor\u001b[0m \u001b[0mprev_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                         ]\n\u001b[0;32m     64\u001b[0m                     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "viterbi_algo_2(test_sentences, count_y_dict, emission_dict)\n",
    "\n",
    "lines = evaluate_results(\"dataset/dev.out\", \"output/dev.p5.out\")\n",
    "res = conlleval.evaluate(lines)\n",
    "print(conlleval.report(res))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('money')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c2d971e524afe961ec82e7f30ef125a0e603d4c2953e19d850a6a14cdbd6eab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
